\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Justin Le},
            pdftitle={Practical Dependent Types in Haskell: Type-Safe Neural Networks},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\title{Practical Dependent Types in Haskell: Type-Safe Neural Networks}
\author{Justin Le}

\begin{document}
\maketitle

\emph{Originally posted on \textbf{\href{https://blog.jle.im/}{in
Code}}.}

Whether you like it or not, programming with dependent types in Haskell
moving slowly but steadily to the mainstream of Haskell programming. In
the current state of Haskell education, dependent types are often
considered topics for ``advanced'' Haskell users. However, I can
definitely foresee a day where the ease of use of modern Haskell
libraries relying on dependent types as well as their ubiquitousness
forces programming with dependent types to be an integral part of
regular intermediate (or even beginner) Haskell education, as much as
Traversable or Maps.

The point of this post is to show some practical examples of using
dependent types in the real world, and to also walk through the ``why''
and high-level philosophy of the way you structure your Haskell
programs. It'll also hopefully instill an intuition of a dependently
typed work flow of ``exploring'' how dependent types can help your
current programs.

The first project in this series will build up to type-safe
\textbf{\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{artificial
neural network}} implementations. Hooray!

\section{Neural Networks}\label{neural-networks}

\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{Artificial
neural networks} have been somewhat of a hot topic in computing
recently. At their core they involve matrix multiplication and
manipulation, so they do seem like a good candidate for a dependent
types. Most importantly, implementations of training algorithms (like
back-propagation) are tricky to implement correctly --- despite being
simple, there are many locations where accidental bugs might pop up when
multiplying the wrong matrices, for example.

It's not always easy to gauge before-the-fact what would or would not be
a good candidate for adding dependent types to, and often times, it can
be considered premature to start off with ``as powerful types as you
can''. So let's walk through programming things with as ``dumb'' types
as possible, and see where types can help.

Edwin Brady calls this process ``type-driven development''. Start
general, recognize the partial functions and red flags, and slowly add
more powerful types.

\subsection{Background}\label{background}

\begin{figure}[htbp]
\centering
\includegraphics{/img/entries/dependent-haskell-1/ffneural.png}
\caption{Feed-forward ANN architecture}
\end{figure}

Here's a quick run through on background for ANN's --- but remember,
this isn't an article on ANN's, so we are going to be glossing over some
of this :) Feel free to explore this further too.

We're going to be implementing a feed-forward neural network, with
back-propagation training. These networks are layers of ``nodes'', each
connected to the each of the nodes of the previous layer. Input goes to
the first layer, which feeds information to the next year, which feeds
it to the next, etc., until the final layer, where we read it off as the
``answer'' that the network is giving us. Layers between the input and
output layers are called \emph{hidden} layers. Every node ``outputs'' a
weighted sum of all of the outputs of the \emph{previous} layer, plus an
always-on ``bias'' term (so that its result can be non-zero even when
all of its inputs are zero). Symbolically, it looks like:

\[
y_j = b_j + \sum_i^m w_{ij} x_i
\]

Or, if we treat the output of a layer and the list of list of weights as
a matrix, we can write it a little cleaner:

\[
\mathbf{y} = \mathbf{b} + W \mathbf{x}
\]

To ``scale'' the result (and to give the system the magical powers of
nonlinearity), we actually apply an ``activation function'' to the
output before passing it down to the next step. We'll be using the
popular \href{https://en.wikipedia.org/wiki/Logistic_function}{logistic
function}, \(f(x) = 1 / (1 + e^{-x})\).

\emph{Training} a network involves picking the right set of weights to
get the network to answer the question you want.

\section{Vanilla Types}\label{vanilla-types}

We can store a network by storing the matrix of of weights and biases
between each layer:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L17-19}
\KeywordTok{data} \DataTypeTok{Weights} \FunctionTok{=} \DataTypeTok{W} \NormalTok{\{}\OtherTok{ wBiases ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Vector} \DataTypeTok{Double}\NormalTok{)  }\CommentTok{-- n}
                 \NormalTok{,}\OtherTok{ wNodes  ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Matrix} \DataTypeTok{Double}\NormalTok{)  }\CommentTok{-- n x m}
                 \NormalTok{\}                              }\CommentTok{-- "m to n" layer}
\end{Highlighting}
\end{Shaded}

Now, a \texttt{Weights} linking an \emph{m}-node layer to an
\emph{n}-node layer has an \emph{n}-dimensional bias vector (one
component for each output) and an \emph{n}-by-\emph{m} node weight
matrix (one column for each output, one row for each input).

(We're using the \texttt{Matrix} type from the awesome
\emph{\href{http://hackage.haskell.org/package/hmatrix}{hmatrix}}
library for performant linear algebra, implemented using blas/lapack
under the hood)

A feed-forward neural network is then just a linked list of these
weights:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L21-27}
\KeywordTok{data} \DataTypeTok{Network}\OtherTok{ ::} \FunctionTok{*} \KeywordTok{where}
    \DataTypeTok{O}\OtherTok{     ::} \FunctionTok{!}\DataTypeTok{Weights}
          \OtherTok{->} \DataTypeTok{Network}
\OtherTok{    (:&~) ::} \FunctionTok{!}\DataTypeTok{Weights}
          \OtherTok{->} \FunctionTok{!}\DataTypeTok{Network}
          \OtherTok{->} \DataTypeTok{Network}
\KeywordTok{infixr} \DecValTok{5} \FunctionTok{:&~}
\end{Highlighting}
\end{Shaded}

Note that we're using
\href{https://en.wikibooks.org/wiki/Haskell/GADT}{GADT} syntax here,
which just lets us define \texttt{Network} (with a kind signature,
\texttt{*}) by providing the type of its \emph{constructors}, \texttt{O}
and \texttt{(:\&\textasciitilde{})}. A network with one input layer, two
inner layers, and one output layer would look like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ih }\FunctionTok{:&~} \NormalTok{hh }\FunctionTok{:&~} \DataTypeTok{O} \NormalTok{ho}
\end{Highlighting}
\end{Shaded}

The first component is the weights from the input to first inner layer,
the second is the weights between the two hidden layers, and the last is
the weights between the last hidden layer and the output layer.

We can write simple procedures, like generating random networks:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L45-55}
\OtherTok{randomWeights ::} \DataTypeTok{MonadRandom} \NormalTok{m }\OtherTok{=>} \DataTypeTok{Int} \OtherTok{->} \DataTypeTok{Int} \OtherTok{->} \NormalTok{m }\DataTypeTok{Weights}
\NormalTok{randomWeights i o }\FunctionTok{=} \KeywordTok{do}
    \NormalTok{s1 }\OtherTok{<-} \NormalTok{getRandom}
    \NormalTok{s2 }\OtherTok{<-} \NormalTok{getRandom}
    \KeywordTok{let} \NormalTok{wB }\FunctionTok{=} \NormalTok{randomVector s1 }\DataTypeTok{Uniform} \NormalTok{o }\FunctionTok{*} \DecValTok{2} \FunctionTok{-} \DecValTok{1}
        \NormalTok{wN }\FunctionTok{=} \NormalTok{uniformSample s2 o (replicate i (}\FunctionTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
    \NormalTok{return }\FunctionTok{$} \DataTypeTok{W} \NormalTok{wB wN}

\OtherTok{randomNet ::} \DataTypeTok{MonadRandom} \NormalTok{m }\OtherTok{=>} \DataTypeTok{Int} \OtherTok{->} \NormalTok{[}\DataTypeTok{Int}\NormalTok{] }\OtherTok{->} \DataTypeTok{Int} \OtherTok{->} \NormalTok{m }\DataTypeTok{Network}
\NormalTok{randomNet i [] o     }\FunctionTok{=}     \DataTypeTok{O} \FunctionTok{<$>} \NormalTok{randomWeights i o}
\NormalTok{randomNet i (h}\FunctionTok{:}\NormalTok{hs) o }\FunctionTok{=} \NormalTok{(}\FunctionTok{:&~}\NormalTok{) }\FunctionTok{<$>} \NormalTok{randomWeights i h }\FunctionTok{<*>} \NormalTok{randomNet h hs o}
\end{Highlighting}
\end{Shaded}

(\texttt{randomVector} and \texttt{uniformSample} are from the
\emph{hmatrix} library, generating random vectors and matrices from a
random \texttt{Int} seed. We configure them to generate them with
numbers between -1 and 1)

And now a function to ``run'' our network on a given input vector,
following the matrix equation we wrote earlier:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L29-43}
\OtherTok{logistic ::} \DataTypeTok{Floating} \NormalTok{a }\OtherTok{=>} \NormalTok{a }\OtherTok{->} \NormalTok{a}
\NormalTok{logistic x }\FunctionTok{=} \DecValTok{1} \FunctionTok{/} \NormalTok{(}\DecValTok{1} \FunctionTok{+} \NormalTok{exp (}\FunctionTok{-}\NormalTok{x))}

\OtherTok{runLayer ::} \DataTypeTok{Weights} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double}
\NormalTok{runLayer (}\DataTypeTok{W} \NormalTok{wB wN) v }\FunctionTok{=} \NormalTok{wB }\FunctionTok{+} \NormalTok{wN }\FunctionTok{#>} \NormalTok{v}

\OtherTok{runNet ::} \DataTypeTok{Network} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double}
\NormalTok{runNet (}\DataTypeTok{O} \NormalTok{w)      }\FunctionTok{!}\NormalTok{v }\FunctionTok{=} \NormalTok{logistic (runLayer w v)}
\NormalTok{runNet (w }\FunctionTok{:&~} \NormalTok{n') }\FunctionTok{!}\NormalTok{v }\FunctionTok{=} \KeywordTok{let} \NormalTok{v' }\FunctionTok{=} \NormalTok{logistic (runLayer w v)}
                       \KeywordTok{in}  \NormalTok{runNet n' v'}
\end{Highlighting}
\end{Shaded}

(\texttt{\#\textgreater{}} is matrix-vector multiplication)

If you're a non-Haskell programmer, this might all seem perfectly fine
and normal, and you probably have only a slightly elevated heart rate.
If you are a Haskell programmer, you are most likely already having
heart attacks. Let's imagine all of the bad things that could happen:

\begin{itemize}
\item
  How do we know that we didn't accidentally mix up the dimensions for
  our implementation of \texttt{randomWeights}? We could have switched
  parameters and be none the wiser.
\item
  How do we even know that each subsequent matrix in the network is
  ``compatible''? We want the outputs of one matrix to line up with the
  inputs of the next, but there's no way to know. It's possible to build
  a bad network, and things will just explode at runtime.
\item
  How do we know the size of vector the network expects? What stops you
  from sending in a bad vector at run-time? We might do runtime-checks,
  but the compiler won't help us.
\item
  How do we verify that we have implemented \texttt{runLayer} and
  \texttt{runNet} in a way that they won't suddenly fail at runtime? We
  write \texttt{l\ \#\textgreater{}\ v}, but how do we know that it's
  even correct\ldots{}what if we forgot to multiply something, or used
  something in the wrong places? We can it prove ourselves, but the
  compiler won't help us.
\end{itemize}

\subsection{Back-propagation}\label{back-propagation}

Now, let's try implementing back-propagation! It's a textbook gradient
descent algorithm. There are
\href{https://en.wikipedia.org/wiki/Backpropagation}{many explanations}
on the internet; the basic idea is that you try to minimize the squared
``error'' of what the neural network outputs for a given input vs.~the
actual expected output. You find the direction of change that minimizes
the error (by finding the derivative), and move that direction. The
implementation of Feed-forward backpropagation is found in many sources
online and in literature, so let's see the implementation in Haskell:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L57-93}
\OtherTok{train ::} \DataTypeTok{Double}           \CommentTok{-- ^ learning rate}
      \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double}    \CommentTok{-- ^ input vector}
      \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double}    \CommentTok{-- ^ target vector}
      \OtherTok{->} \DataTypeTok{Network}          \CommentTok{-- ^ network to train}
      \OtherTok{->} \DataTypeTok{Network}
\NormalTok{train rate x0 target }\FunctionTok{=} \NormalTok{fst }\FunctionTok{.} \NormalTok{go x0}
  \KeywordTok{where}
\OtherTok{    go ::} \DataTypeTok{Vector} \DataTypeTok{Double}    \CommentTok{-- ^ input vector}
       \OtherTok{->} \DataTypeTok{Network}          \CommentTok{-- ^ network to train}
       \OtherTok{->} \NormalTok{(}\DataTypeTok{Network}\NormalTok{, }\DataTypeTok{Vector} \DataTypeTok{Double}\NormalTok{)}
    \NormalTok{go }\FunctionTok{!}\NormalTok{x (}\DataTypeTok{O} \NormalTok{w}\FunctionTok{@}\NormalTok{(}\DataTypeTok{W} \NormalTok{wB wN))}
        \FunctionTok{=} \KeywordTok{let} \NormalTok{y    }\FunctionTok{=} \NormalTok{runLayer w x}
              \NormalTok{o    }\FunctionTok{=} \NormalTok{logistic y}
              \CommentTok{-- the gradient (how much y affects the error)}
              \CommentTok{--   (logistic' is the derivative of logistic)}
              \NormalTok{dEdy }\FunctionTok{=} \NormalTok{logistic' y }\FunctionTok{*} \NormalTok{(o }\FunctionTok{-} \NormalTok{target)}
              \CommentTok{-- new bias weights and node weights}
              \NormalTok{wB'  }\FunctionTok{=} \NormalTok{wB }\FunctionTok{-} \NormalTok{scale rate dEdy}
              \NormalTok{wN'  }\FunctionTok{=} \NormalTok{wN }\FunctionTok{-} \NormalTok{scale rate (dEdy }\OtherTok{`outer`} \NormalTok{x)}
              \NormalTok{w'   }\FunctionTok{=} \DataTypeTok{W} \NormalTok{wB' wN'}
              \CommentTok{-- bundle of derivatives for next step}
              \NormalTok{dWs  }\FunctionTok{=} \NormalTok{tr wN }\FunctionTok{#>} \NormalTok{dEdy}
          \KeywordTok{in}  \NormalTok{(}\DataTypeTok{O} \NormalTok{w', dWs)}
    \NormalTok{go }\FunctionTok{!}\NormalTok{x (w}\FunctionTok{@}\NormalTok{(}\DataTypeTok{W} \NormalTok{wB wN) }\FunctionTok{:&~} \NormalTok{n)}
        \FunctionTok{=} \KeywordTok{let} \NormalTok{y          }\FunctionTok{=} \NormalTok{runLayer w x}
              \NormalTok{o          }\FunctionTok{=} \NormalTok{logistic y}
              \CommentTok{-- get dWs', bundle of derivatives from rest of the net}
              \NormalTok{(n', dWs') }\FunctionTok{=} \NormalTok{go o n}
              \CommentTok{-- the gradient (how much y affects the error)}
              \NormalTok{dEdy       }\FunctionTok{=} \NormalTok{logistic' y }\FunctionTok{*} \NormalTok{dWs'}
              \CommentTok{-- new bias weights and node weights}
              \NormalTok{wB'  }\FunctionTok{=} \NormalTok{wB }\FunctionTok{-} \NormalTok{scale rate dEdy}
              \NormalTok{wN'  }\FunctionTok{=} \NormalTok{wN }\FunctionTok{-} \NormalTok{scale rate (dEdy }\OtherTok{`outer`} \NormalTok{x)}
              \NormalTok{w'   }\FunctionTok{=} \DataTypeTok{W} \NormalTok{wB' wN'}
              \CommentTok{-- bundle of derivatives for next step}
              \NormalTok{dWs  }\FunctionTok{=} \NormalTok{tr wN }\FunctionTok{#>} \NormalTok{dEdy}
          \KeywordTok{in}  \NormalTok{(w' }\FunctionTok{:&~} \NormalTok{n', dWs)}
\end{Highlighting}
\end{Shaded}

The algorithm computes the \emph{updated} network by recursively
updating the layers, from the output layer all the way up to the input
layer. At every step it returns the updated layer/network, as well as a
bundle of derivatives for the next layer to use to calculate its descent
direction. At the output layer, all it needs to calculate the direction
of descent is just \texttt{o\ -\ targ}, the target. At the inner layers,
it has to use the \texttt{dWs} bundle to figure it out.

Writing this is a bit of a struggle. I actually implemented this
incorrectly several times before writing it as you see here. The type
system doesn't help you like it normally does in Haskell, and you can't
really use parametricity to help you write your code like normal
Haskell. Everything is monomorphic, and everything multiplies with
everything else. You don't have any hits about what to multiply with
what at any point in time. It's like all of the bad things mentioned
before, but amplified.

In short, you're leaving yourself open to many potential bugs\ldots{}and
the compiler doesn't help you write your code at all! This is the
nightmare of every Haskell programmer. There must be a better way!

\subsubsection{Putting it to the test}\label{putting-it-to-the-test}

Pretty much the only way you can verify this code is to test it out on
example cases. In the
\href{https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs}{source
file} I have \texttt{main} test out the backprop, training a network on
a 2D function that was ``on'' for two small circles and ``off''
everywhere else (A nice cute non-linearly-separable function to test our
network on). We basically train the network to be able to recognize the
two-circle pattern. I implemented a simple printing function and tested
the trained network on a grid:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{stack} \NormalTok{install hmatrix MonadRandom}
\NormalTok{$ }\KeywordTok{stack} \NormalTok{ghc -- -O2 ./NetworkUntyped.hs}
\NormalTok{$ }\KeywordTok{./NetworkUntyped}
\CommentTok{# Training network...}
\CommentTok{#}
\CommentTok{#}
\CommentTok{#            .=########=}
\CommentTok{#          .##############.}
\CommentTok{#          ################}
\CommentTok{#          ################}
\CommentTok{#          .##############-}
\CommentTok{#            .###########}
\CommentTok{#                 ...             ...}
\CommentTok{#                             -##########.}
\CommentTok{#                           -##############.}
\CommentTok{#                           ################}
\CommentTok{#                           ################}
\CommentTok{#                            =############=}
\CommentTok{#                              .#######=.}
\CommentTok{#}
\CommentTok{#}
\end{Highlighting}
\end{Shaded}

Not too bad! The network learned to recognize the circles. But, I was
basically forced to resort to unit testing to ensure my code was
correct. Let's see if we can do better.

\subsection{The Call of Types}\label{the-call-of-types}

Before we go on to the ``typed'' version of our program, let's take a
step back and look at some big checks you might want to ask yourself
after you write code in Haskell.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Are any of my functions partial, or implemented using partial
  functions?
\item
  How could I have written things that are \emph{incorrect}, and yet
  still type check? Where does the compiler \emph{not} help me by
  restricting my choices?
\end{enumerate}

Both of these questions usually yield some truth about the code you
write and the things you should worry about. As a Haskeller, they should
always be at the back of your mind!

Looking back at our untyped implementation, we notice some things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Literally every single function we wrote is partial. (Okay, maybe not
  \emph{literally} every, but almost every) If we had passed in the
  incorrectly sized matrix/vector, or stored mismatched vectors in our
  network, everything would fall apart.
\item
  There are billions of ways we could have implemented our functions
  where they would still typechecked. We could multiply mismatched
  matrices, or forget to multiply a matrix, etc.
\end{enumerate}

\section{With Static Types}\label{with-static-types}

\subsection{Networks}\label{networks}

Gauging our potential problems, it seems like the first major class of
bugs we can address is improperly sized and incompatible matrices. If
the compiler always made sure we used compatible matrices, we can avoid
bugs at compile-time, and we also can get a friendly helper when we
write programs (by knowing what works with what, and what we need were)

Let's write a \texttt{Weights} type that tells you the size of its
output and the input it expects. Let's have, say, a
\texttt{Weights\ 10\ 5} be a set of weights that takes you from a layer
of 10 nodes to a layer of 5 nodes. \texttt{w\ ::\ Weights\ 4\ 6} would
take you from a layer of 4 nodes to a layer of 6 nodes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L22-24}
\KeywordTok{data} \DataTypeTok{Weights} \NormalTok{i o }\FunctionTok{=} \DataTypeTok{W} \NormalTok{\{}\OtherTok{ wBiases ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{R} \NormalTok{o)}
                     \NormalTok{,}\OtherTok{ wNodes  ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{L} \NormalTok{o i)}
                     \NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We're using the \texttt{Numeric.LinearAlgebra.Static} module from
\emph{\href{http://hackage.haskell.org/package/hmatrix}{hmatrix}}, which
offers matrix and vector types with their size in their types: an
\texttt{R\ 5} is a vector of Doubles with 5 elements, and a
\texttt{L\ 3\ 6} is a 3x6 vector of Doubles.

The \texttt{Static} module relies on the \texttt{KnownNat} mechanism
that GHC offers. Almost all operations in the library require a
\texttt{KnownNat} constraint on the type-level Nats --- for example, you
can take the dot product of two vectors with
\texttt{dot\ ::\ KnownNat\ n\ =\textgreater{}\ R\ n\ -\textgreater{}\ R\ n\ -\textgreater{}\ Double}.
The \texttt{KnownNat} constraint allows the functions to \emph{use} the
information in the size parameter (the \texttt{Integer} it represents)
at run-time. (More on this later!)

Our network type for this post will be something like
\texttt{Network\ 10\ \textquotesingle{}{[}7,5,3{]}\ 2}: Take 10 inputs,
return 2 outputs --- and internally, have hidden layers of size 7, 5,
and 3. (The \texttt{\textquotesingle{}{[}7,5,3{]}} is a type-level list
of Nats; the optional \texttt{\textquotesingle{}} apostrophe is just for
our own benefit to distinguish it from a value-level list of integers.)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L26-33}
\KeywordTok{data} \DataTypeTok{Network}\OtherTok{ ::} \DataTypeTok{Nat} \OtherTok{->} \NormalTok{[}\DataTypeTok{Nat}\NormalTok{] }\OtherTok{->} \DataTypeTok{Nat} \OtherTok{->} \FunctionTok{*} \KeywordTok{where}
    \DataTypeTok{O}\OtherTok{     ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Weights} \NormalTok{i o)}
          \OtherTok{->} \DataTypeTok{Network} \NormalTok{i }\CharTok{'[] o}
\OtherTok{    (:&~) ::} \DataTypeTok{KnownNat} \NormalTok{h}
          \OtherTok{=>} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Weights} \NormalTok{i h)}
          \OtherTok{->} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Network} \NormalTok{h hs o)}
          \OtherTok{->} \DataTypeTok{Network} \NormalTok{i (h }\CharTok{': hs) o}
\KeywordTok{infixr} \DecValTok{5} \FunctionTok{:&~}
\end{Highlighting}
\end{Shaded}

We use GADT syntax here again. The \emph{kind signature} of the type
constructor means that the \texttt{Network} type constructor takes three
inputs: a \texttt{Nat} (type-level numeral, like \texttt{10} or
\texttt{5}), list of \texttt{Nat}s, and another \texttt{Nat} (the input,
hidden layers, and output sizes). Let's go over the two constructors.

\begin{itemize}
\item
  The \texttt{O} constructor takes a \texttt{Weights\ i\ o} and returns
  a \texttt{Network\ i\ \textquotesingle{}{[}{]}\ o}. That is, if your
  network is just weights from \texttt{i} inputs to \texttt{o} outputs,
  your network itself just takes \texttt{i} inputs and returns
  \texttt{o} outputs, with no hidden layers.
\item
  The \texttt{(:\&\textasciitilde{})} constructor takes a
  \texttt{Network\ h\ hs\ o} -- a network with \texttt{h} inputs and
  \texttt{o} outputs -- and ``conses'' an extra input layer in front. If
  you give it a \texttt{Weights\ i\ h}, its outputs fit perfectly into
  the inputs of the subnetwork, and you get a
  \texttt{Network\ i\ (h\ \textquotesingle{}:\ hs)\ o}.
  (\texttt{(\textquotesingle{}:)}, or \texttt{(:)}, is the same as
  normal \texttt{(:)}, but is for type-level lists. The apostrophe is
  optional here too, but it's just nice to be able to visually
  distinguish the two)

  We add a \texttt{KnownNat} constraint on the \texttt{h}, so that
  whenever you pattern match on \texttt{w\ :\&\textasciitilde{}\ net},
  you automatically get a \texttt{KnownNat} constraint for the input
  size of \texttt{net} that the \emph{hmatrix} library can use.
\end{itemize}

We can still construct them the same way:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- given:}
\OtherTok{ho ::} \DataTypeTok{Weights}  \DecValTok{4} \DecValTok{2}
\OtherTok{hh ::} \DataTypeTok{Weights}  \DecValTok{7} \DecValTok{4}
\OtherTok{ih ::} \DataTypeTok{Weights} \DecValTok{10} \DecValTok{7}

\CommentTok{-- we have:}
              \DataTypeTok{O}\OtherTok{ ho      ::} \DataTypeTok{Network}  \DecValTok{4} \CharTok{'[] 2}
       \NormalTok{hh }\FunctionTok{:&~} \DataTypeTok{O}\OtherTok{ ho      ::} \DataTypeTok{Network}  \DecValTok{7} \CharTok{'[4] 2}
\NormalTok{ih }\FunctionTok{:&~} \NormalTok{hh }\FunctionTok{:&~} \DataTypeTok{O}\OtherTok{ ho      ::} \DataTypeTok{Network} \DecValTok{10} \CharTok{'[7,4] 2}
\end{Highlighting}
\end{Shaded}

Note that the shape of the constructors requires all of the weight
vectors to ``fit together''. \texttt{ih\ :\&\textasciitilde{}\ O\ ho}
would be a type error (feeding a 7-output layer to a 4-input layer).
Now, if we ever pattern match on \texttt{:\&\textasciitilde{}}, we know
that the resulting matrices and vectors are compatible!

One neat thing is that this approach is also self-documenting. I don't
need to specify what the dimensions are in the docs and trust the users
to read it and obey it. The types tell them! And if they don't listen,
they get a compiler error! (You should, of course, still provide
reasonable documentation. But, in this case, the compiler actually
enforces your documentation's statements!)

Generating random weights and networks is even nicer now:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L57-64}
\OtherTok{randomWeights ::} \NormalTok{(}\DataTypeTok{MonadRandom} \NormalTok{m, }\DataTypeTok{KnownNat} \NormalTok{i, }\DataTypeTok{KnownNat} \NormalTok{o)}
              \OtherTok{=>} \NormalTok{m (}\DataTypeTok{Weights} \NormalTok{i o)}
\NormalTok{randomWeights }\FunctionTok{=} \KeywordTok{do}
    \NormalTok{s1 }\OtherTok{<-} \NormalTok{getRandom}
    \NormalTok{s2 }\OtherTok{<-} \NormalTok{getRandom}
    \KeywordTok{let} \NormalTok{wB }\FunctionTok{=} \NormalTok{randomVector s1 }\DataTypeTok{Uniform} \FunctionTok{*} \DecValTok{2} \FunctionTok{-} \DecValTok{1}
        \NormalTok{wN }\FunctionTok{=} \NormalTok{uniformSample s2 (}\FunctionTok{-}\DecValTok{1}\NormalTok{) }\DecValTok{1}
    \NormalTok{return }\FunctionTok{$} \DataTypeTok{W} \NormalTok{wB wN}
\end{Highlighting}
\end{Shaded}

Notice that the \texttt{Static} versions of \texttt{randomVector} and
\texttt{uniformSample} don't actually require the size of the
vector/matrix you want as an input -- they just use type inference to
figure out what size you want! This is the same process that
\texttt{read} uses to figure out what type of thing you want to return.
You would use \texttt{randomVector\ s\ Uniform\ ::\ R\ 10}, and type
inference would give you a 10-element vector the same way
\texttt{read\ "hello"\ ::\ Int} would give you an \texttt{Int}.

Here's something important: note that it's much harder to implement this
incorrectly. Before, you could give the matrix the wrong dimensions
(maybe you flipped the parameters?), or gave the wrong parameter to the
vector generator.

But here, you are guaranteed/forced to return the correctly sized
vectors and matrices. In fact, you \emph{don't even have to worry} about
it --- it's handled automatically by the magic of type
inference\footnote{Thank you based Hindley-Milner.}! I consider this a
very big victory. One of the whole points of types is to give you less
to ``worry about'', as a programmer. Here, we completely eliminate an
\emph{entire dimension} of programmer concern.

\subsection{Singletons and Induction}\label{singletons-and-induction}

The code for the updated \texttt{randomNet} takes a bit of background to
understand, so let's take a quick detour through the concepts of
singletons and induction on data types.

Let's say we want to construct a
\texttt{Network\ 4\ \textquotesingle{}{[}3,2{]}\ 1} by implementing an
algorithm that can create any \texttt{Network\ i\ hs\ o}. In true
Haskell fashion, we do this recursively (``inductively''). After all, we
know how to make a \texttt{Network\ i\ \textquotesingle{}{[}{]}\ o}
(just \texttt{O\ \textless{}\$\textgreater{}\ randomWieights}), and we
know how to create a
\texttt{Network\ i\ (h\ \textquotesingle{}:\ hs)\ o} if we had a
\texttt{Network\ h\ hs\ o} (just use \texttt{(:\&\textasciitilde{})}
with a \texttt{randomWeights}). Now all we have to do is just ``pattern
match'' on the type-level list, and\ldots{}

Oh wait. We can't directly pattern match on lists like that in Haskell.
But what we \emph{can} do is move the list from the type level to the
value level using \emph{singletons}. The
\emph{\href{http://hackage.haskell.org/package/typelits-witnesses}{typelits-witnesses}}
library offers a handy singleton for just this job. If you have a type
level list of nats, you get a \texttt{KnowNats\ ns} constraint. This
lets you create a \texttt{NatList}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data} \DataTypeTok{NatList}\OtherTok{ ::} \NormalTok{[}\DataTypeTok{Nat}\NormalTok{] }\OtherTok{->} \FunctionTok{*} \KeywordTok{where}
    \NormalTok{Ø}\DataTypeTok{NL}\OtherTok{   ::} \DataTypeTok{NatList} \CharTok{'[]}
\OtherTok{    (:<#) ::} \NormalTok{(}\DataTypeTok{KnownNat} \NormalTok{n, }\DataTypeTok{KnownNats} \NormalTok{ns)}
          \OtherTok{=>} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Proxy} \NormalTok{n) }\OtherTok{->} \FunctionTok{!}\NormalTok{(}\DataTypeTok{NatList} \NormalTok{ns) }\OtherTok{->} \DataTypeTok{NatList} \NormalTok{(n }\CharTok{': ns)}

\KeywordTok{infixr} \DecValTok{5} \FunctionTok{:<#}
\end{Highlighting}
\end{Shaded}

Basically, a \texttt{NatList\ \textquotesingle{}{[}1,2,3{]}} is
\texttt{p1\ :\textless{}\#\ p2\ :\textless{}\#\ p3\ :\textless{}\#\ ØNL},
where \texttt{p1\ ::\ Proxy\ 1}, \texttt{p2\ ::\ Proxy\ 2}, and
\texttt{p3\ ::\ Proxy\ 3}. (Remember, \texttt{data\ Proxy\ a\ =\ Proxy};
\texttt{Proxy} is like \texttt{()} but with an extra phantom type
parameter)

We can spontaneously generate a \texttt{NatList} for any type-level Nat
list with
\texttt{natList\ ::\ KnownNats\ ns\ =\textgreater{}\ NatList\ ns}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ghci}\FunctionTok{>}\OtherTok{ natList ::} \DataTypeTok{NatList} \CharTok{'[1,2,3]}
\DataTypeTok{Proxy} \FunctionTok{:<#} \DataTypeTok{Proxy} \FunctionTok{:<#} \DataTypeTok{Proxy} \FunctionTok{:<#} \NormalTok{Ø}\DataTypeTok{NL}
\CommentTok{-- ^         ^         ^}
\CommentTok{-- `-- :: Pro|xy 1     |}
\CommentTok{--           `-- :: Pro|xy 2}
\CommentTok{--                     `-- :: Proxy 3}
\end{Highlighting}
\end{Shaded}

Now that we have an actual value-level \emph{structure} (the list of
\texttt{Proxy}s), we can now essentially ``pattern match'' on
\texttt{hs}, the type --- if it's empty, we'll get the \texttt{ØNL}
constructor when we use \texttt{natList}, otherwise we'll get the
\texttt{(:\textless{}\#)} constructor, etc.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L66-70}
\OtherTok{randomNet ::} \NormalTok{forall m i hs o}\FunctionTok{.} \NormalTok{(}\DataTypeTok{MonadRandom} \NormalTok{m, }\DataTypeTok{KnownNat} \NormalTok{i, }\DataTypeTok{KnownNats} \NormalTok{hs, }\DataTypeTok{KnownNat} \NormalTok{o)}
          \OtherTok{=>} \NormalTok{m (}\DataTypeTok{Network} \NormalTok{i hs o)}
\NormalTok{randomNet }\FunctionTok{=} \KeywordTok{case}\OtherTok{ natsList ::} \DataTypeTok{NatList} \NormalTok{hs }\KeywordTok{of}
              \NormalTok{Ø}\DataTypeTok{NL}     \OtherTok{->} \DataTypeTok{O}     \FunctionTok{<$>} \NormalTok{randomWeights}
              \NormalTok{_ }\FunctionTok{:<#} \NormalTok{_ }\OtherTok{->} \NormalTok{(}\FunctionTok{:&~}\NormalTok{) }\FunctionTok{<$>} \NormalTok{randomWeights }\FunctionTok{<*>} \NormalTok{randomNet}
\end{Highlighting}
\end{Shaded}

(Note that we need \texttt{ScopedTypeVariables} and explicit
\texttt{forall} so that can say \texttt{NatList\ hs} in the body of the
declaration.)

The reason why \texttt{NatList} and \texttt{:\textless{}\#} works for
this is that its constructors \emph{come with ``proofs''} that the
head's type has a \texttt{KnownNat} constraint and the tail's type has a
\texttt{KnownNats} one. It's a part of the GADT declaration. If you ever
pattern match on \texttt{p\ :\textless{}\#\ ns}, you get a
\texttt{KnownNat\ n} constraint (that \texttt{randomWeights} uses) for
the \texttt{p\ ::\ Proxy\ n}, and also a \texttt{KnownNats\ ns}
constraint (that the recursive call to \texttt{randomNet} uses).

This is a common pattern in dependent Haskell: ``building up'' a
value-level singleton \emph{structure} from a type that we want (often
done through typeclasses like \texttt{KnownNats}) and then inductively
piggybacking on that structure's constructors to build the thing you
\emph{really} want (called ``elimination''). Here, we use
\texttt{KnownNats\ hs} to build our \texttt{NatList\ hs} structure, and
use/``eliminate'' that structure to create our
\texttt{Network\ i\ hs\ o}.

Along the way, the singletons and the typeclasses and the types play an
intricate dance. \texttt{randomWeights} needed a \texttt{KnownNat}
constraint. Where did it \emph{come} from?

\subsubsection{On Typeclasses}\label{on-typeclasses}

\texttt{natList} uses the \texttt{KnownNat\ n} to construct the
\texttt{NatList\ ns} (because any time you use
\texttt{(:\textless{}\#)}, you have/need a \texttt{KnownNat} instance in
scope). Then, when you pattern match on the \texttt{(:\textless{}\#)} in
\texttt{randomNet}, you ``release'' the \texttt{KnownNat\ n} that was
stuffed in there by \texttt{natList}.

People say that pattern matching on \texttt{(:\textless{}\#)} gives you
a ``context'' in that case-statement-branch where \texttt{KnownNat\ n}
is in scope and satisfied. But sometimes it helps to think of it in the
way we just did --- the instance \emph{itself} is actually a ``thing''
that gets passed around through GADT constructors/deconstructors. The
\texttt{KnownNat} \emph{instance} gets put \emph{into}
\texttt{:\textless{}\#} by \texttt{natList}, and is then taken
\emph{out} in the pattern match so that \texttt{randomWeights} can use
it. (When we match on \texttt{\_\ :\textless{}\#\ \_}, we really are
saying that we don't care about the ``normal'' contents --- we just want
the typeclass instances that the constructor is hiding!)

At a high-level, you can see that this is really no different than just
having a plain old \texttt{Integer} that you ``put in'' to the
constructor (as an extra field), and which you then take out if you
pattern match on it. Really, every time you see
\texttt{KnownNat\ n\ =\textgreater{}\ ..}, you can think of it as an
\texttt{Integer\ -\textgreater{}\ ..} (because all the typeclass is is a
way to get an \texttt{Integer} out of it with \texttt{natVal}).
\texttt{(:\textless{}\#)} requiring a
\texttt{KnownNat\ n\ =\textgreater{}} put into it is really the same as
requiring an \texttt{Integer} in it, which the act of pattern-matching
can then take out.

The difference is that GHC and the compiler can now ``track'' these at
compile-time to give you rudimentary checks on how your Nat's act
together on the type level, allowing it to catch mismatches with
compile-time checks instead of run-time checks.

\subsection{Running with it}\label{running-with-it}

So now, you can use
\texttt{randomNet\ ::\ IO\ (Network\ 5\ \textquotesingle{}{[}4,3{]}\ 2)}
to get a random network of the desired dimensions!

Can we just pause right here to just appreciate how awesome it is that
we can generate random networks of whatever size we want by \emph{just
requesting something by its type}? Our implementation is also
\emph{guaranteed} to have the right sized matrices\ldots{}no worrying
about using the right size parameters for the right matrix in the right
oder. GHC does it for you automatically!

The code for \emph{running} the nets is actually literally identical
from before:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L43-55}
\OtherTok{runLayer ::} \NormalTok{(}\DataTypeTok{KnownNat} \NormalTok{i, }\DataTypeTok{KnownNat} \NormalTok{o)}
         \OtherTok{=>} \DataTypeTok{Weights} \NormalTok{i o}
         \OtherTok{->} \DataTypeTok{R} \NormalTok{i}
         \OtherTok{->} \DataTypeTok{R} \NormalTok{o}
\NormalTok{runLayer (}\DataTypeTok{W} \NormalTok{wB wN) v }\FunctionTok{=} \NormalTok{wB }\FunctionTok{+} \NormalTok{wN }\FunctionTok{#>} \NormalTok{v}

\OtherTok{runNet ::} \NormalTok{(}\DataTypeTok{KnownNat} \NormalTok{i, }\DataTypeTok{KnownNat} \NormalTok{o)}
       \OtherTok{=>} \DataTypeTok{Network} \NormalTok{i hs o}
       \OtherTok{->} \DataTypeTok{R} \NormalTok{i}
       \OtherTok{->} \DataTypeTok{R} \NormalTok{o}
\NormalTok{runNet (}\DataTypeTok{O} \NormalTok{w)      }\FunctionTok{!}\NormalTok{v }\FunctionTok{=} \NormalTok{logistic (runLayer w v)}
\NormalTok{runNet (w }\FunctionTok{:&~} \NormalTok{n') }\FunctionTok{!}\NormalTok{v }\FunctionTok{=} \KeywordTok{let} \NormalTok{v' }\FunctionTok{=} \NormalTok{logistic (runLayer w v)}
                       \KeywordTok{in}  \NormalTok{runNet n' v'}
\end{Highlighting}
\end{Shaded}

But now, we get the assurance that the matrices and vectors all fit
each-other, at compile-time. GHC basically writes our code for us. The
operations all demand vectors and matrices that ``fit together'', so you
can only ever multiply a matrix by a properly sized vector.

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{(+)  ::} \DataTypeTok{KnownNat} \NormalTok{n}
     \OtherTok{=>} \DataTypeTok{R} \NormalTok{n }\OtherTok{->} \DataTypeTok{R} \NormalTok{n }\OtherTok{->} \DataTypeTok{R} \NormalTok{n}
\OtherTok{(#>) ::} \NormalTok{(}\DataTypeTok{KnownNat} \NormalTok{n, }\DataTypeTok{KnownNat} \NormalTok{m)}
     \OtherTok{=>} \DataTypeTok{L} \NormalTok{n m }\OtherTok{->} \DataTypeTok{R} \NormalTok{m }\OtherTok{->} \DataTypeTok{R} \NormalTok{n}

\OtherTok{logistic ::} \DataTypeTok{KnownNat} \NormalTok{n}
         \OtherTok{=>} \DataTypeTok{R} \NormalTok{n }\OtherTok{->} \DataTypeTok{R} \NormalTok{n}
\end{Highlighting}
\end{Shaded}

The source code is the same from before, so there isn't any extra
overhead in annotation. The correctness proofs and guarantees basically
come without any extra work --- they're free!

Our back-prop algorithm is ported pretty nicely too:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L72-110}
\OtherTok{train ::} \NormalTok{forall i hs o}\FunctionTok{.} \NormalTok{(}\DataTypeTok{KnownNat} \NormalTok{i, }\DataTypeTok{KnownNat} \NormalTok{o)}
      \OtherTok{=>} \DataTypeTok{Double}           \CommentTok{-- ^ learning rate}
      \OtherTok{->} \DataTypeTok{R} \NormalTok{i              }\CommentTok{-- ^ input vector}
      \OtherTok{->} \DataTypeTok{R} \NormalTok{o              }\CommentTok{-- ^ target vector}
      \OtherTok{->} \DataTypeTok{Network} \NormalTok{i hs o   }\CommentTok{-- ^ network to train}
      \OtherTok{->} \DataTypeTok{Network} \NormalTok{i hs o}
\NormalTok{train rate x0 target }\FunctionTok{=} \NormalTok{fst }\FunctionTok{.} \NormalTok{go x0}
  \KeywordTok{where}
\OtherTok{    go  ::} \NormalTok{forall j js}\FunctionTok{.} \DataTypeTok{KnownNat} \NormalTok{j}
        \OtherTok{=>} \DataTypeTok{R} \NormalTok{j              }\CommentTok{-- ^ input vector}
        \OtherTok{->} \DataTypeTok{Network} \NormalTok{j js o   }\CommentTok{-- ^ network to train}
        \OtherTok{->} \NormalTok{(}\DataTypeTok{Network} \NormalTok{j js o, }\DataTypeTok{R} \NormalTok{j)}
    \NormalTok{go }\FunctionTok{!}\NormalTok{x (}\DataTypeTok{O} \NormalTok{w}\FunctionTok{@}\NormalTok{(}\DataTypeTok{W} \NormalTok{wB wN))}
        \FunctionTok{=} \KeywordTok{let} \NormalTok{y    }\FunctionTok{=} \NormalTok{runLayer w x}
              \NormalTok{o    }\FunctionTok{=} \NormalTok{logistic y}
              \CommentTok{-- the gradient (how much y affects the error)}
              \CommentTok{--   (logistic' is the derivative of logistic)}
              \NormalTok{dEdy }\FunctionTok{=} \NormalTok{logistic' y }\FunctionTok{*} \NormalTok{(o }\FunctionTok{-} \NormalTok{target)}
              \CommentTok{-- new bias weights and node weights}
              \NormalTok{wB'  }\FunctionTok{=} \NormalTok{wB }\FunctionTok{-} \NormalTok{konst rate }\FunctionTok{*} \NormalTok{dEdy}
              \NormalTok{wN'  }\FunctionTok{=} \NormalTok{wN }\FunctionTok{-} \NormalTok{konst rate }\FunctionTok{*} \NormalTok{(dEdy }\OtherTok{`outer`} \NormalTok{x)}
              \NormalTok{w'   }\FunctionTok{=} \DataTypeTok{W} \NormalTok{wB' wN'}
              \CommentTok{-- bundle of derivatives for next step}
              \NormalTok{dWs  }\FunctionTok{=} \NormalTok{tr wN }\FunctionTok{#>} \NormalTok{dEdy}
          \KeywordTok{in}  \NormalTok{(}\DataTypeTok{O} \NormalTok{w', dWs)}
    \NormalTok{go }\FunctionTok{!}\NormalTok{x (w}\FunctionTok{@}\NormalTok{(}\DataTypeTok{W} \NormalTok{wB wN) }\FunctionTok{:&~} \NormalTok{n)}
        \FunctionTok{=} \KeywordTok{let} \NormalTok{y          }\FunctionTok{=} \NormalTok{runLayer w x}
              \NormalTok{o          }\FunctionTok{=} \NormalTok{logistic y}
              \CommentTok{-- get dWs', bundle of derivatives from rest of the net}
              \NormalTok{(n', dWs') }\FunctionTok{=} \NormalTok{go o n}
              \CommentTok{-- the gradient (how much y affects the error)}
              \NormalTok{dEdy       }\FunctionTok{=} \NormalTok{logistic' y }\FunctionTok{*} \NormalTok{dWs'}
              \CommentTok{-- new bias weights and node weights}
              \NormalTok{wB'  }\FunctionTok{=} \NormalTok{wB }\FunctionTok{-} \NormalTok{konst rate }\FunctionTok{*} \NormalTok{dEdy}
              \NormalTok{wN'  }\FunctionTok{=} \NormalTok{wN }\FunctionTok{-} \NormalTok{konst rate }\FunctionTok{*} \NormalTok{(dEdy }\OtherTok{`outer`} \NormalTok{x)}
              \NormalTok{w'   }\FunctionTok{=} \DataTypeTok{W} \NormalTok{wB' wN'}
              \CommentTok{-- bundle of derivatives for next step}
              \NormalTok{dWs  }\FunctionTok{=} \NormalTok{tr wN }\FunctionTok{#>} \NormalTok{dEdy}
          \KeywordTok{in}  \NormalTok{(w' }\FunctionTok{:&~} \NormalTok{n', dWs)}
\end{Highlighting}
\end{Shaded}

It's pretty much again an exact copy-and-paste, but now with GHC
checking to make sure everything fits together in our implementation.

One thing that's hard for me to convey here without walking through the
implementation step-by-step is how much the types \emph{help you} in
writing this code.

Before starting writing a back-prop implementation without the help of
types, I'd probably be a bit concerned. I mentioned earlier that writing
the untyped version was no fun at all. But, with the types, writing the
implementation became a \emph{joy} again. And, you have the help of
\emph{hole driven development}, too.

If you need, say, an \texttt{R\ n}, there might be only one way go get
it --- only one function that returns it. If you have something that you
need to combine with something you don't know about, you can use typed
holes (\texttt{\_}) and GHC will give you a list of all the values you
have in scope that can fit there. Your programs basically write
themselves!

The more you can restrict the implementations of your functions with
your types, the more of a joy programming in Haskell is. Things fit
together and fall together before your eyes\ldots{}and the best part is
that if they're wrong, the compiler will nudge you gently into the
correct direction.

The most stressful part of programming happens when you have to
tenuously hold a complex and fragile network of ideas and constraints in
your brain, and any slight distraction or break in focus causes
everything to crash down in your mind. Over time, people have began to
believe that this is ``normal'', and a sign of a good programmer. Don't
believe this lie --- it's not! A good programming experience involves
maintaining as \emph{little} in your head as possible, and letting the
compiler handle remembering/checking the rest!

\subsubsection{The final test}\label{the-final-test}

You can download the
\href{https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs}{typed
network} source code and run it yourself. Again, the \texttt{main}
method is written identically to that of the other file, and tests the
identical function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{$ }\KeywordTok{stack} \NormalTok{install hmatrix MonadRandom typelits-witnesses}
\NormalTok{$ }\KeywordTok{stack} \NormalTok{ghc -- -O2 ./NetworkTyped.hs}
\NormalTok{$ }\KeywordTok{./NetworkTyped}
\CommentTok{# Training network...}
\CommentTok{#}
\CommentTok{#}
\CommentTok{#             -#########-}
\CommentTok{#           -#############=}
\CommentTok{#          -###############-}
\CommentTok{#          =###############=}
\CommentTok{#           ##############=.}
\CommentTok{#            .##########=.}
\CommentTok{#                               .==#=-}
\CommentTok{#                            -###########-}
\CommentTok{#                           =##############.}
\CommentTok{#                          .###############=}
\CommentTok{#                           =##############-}
\CommentTok{#                            =############-}
\CommentTok{#                              -######=-.}
\CommentTok{#}
\CommentTok{#}
\end{Highlighting}
\end{Shaded}

\end{document}
