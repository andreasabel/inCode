<!DOCTYPE HTML>
<html><head><title>A Purely Functional Typed Approach to Trainable Models (Part 2) · in Code</title><meta name="description" content="Weblog of Justin Le, covering his various adventures in programming and explorations in the vast worlds of computation physics, and knowledge."><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><meta name="flattr:id" content="3p9jqr"><meta property="og:site_name" content="in Code"><meta property="og:description" content="Welcome back! We’re going to be jumping right back into describing a vision of a purely functional typed approach to writing trainable models using differentiable programming. If you’re just joining us, be sure to check out Part 1 first! In the last post, we looked at models as “question and answer” systems. We described them as essentially being functions of type 
f : P \rightarrow (A \rightarrow B)
 Where, for f_p(x) = y, you have a “question” x : A and are looking for an “answer” y : B. Picking a different p : P will give a different A \rightarrow B function. We claimed that training a model was finding just the right p to use with the model to yield the right A \rightarrow B function that models your situation. We then noted that if you have a set of (a, b) observations, and your function is differentiable, you can find the gradient of p with respect to the error of your model on each observation, which tells you how to nudge a given p in order to reduce how wrong your model is for that observation. By repeatedly making observations and taking those nudges, you can arrive at a suitable p to model any situation. This is great if we consider a model as “question and answer”, but sometimes things don’t fit so cleanly. Today, we’re going to be looking at a whole different type of model (“time series” models) and see how they are different, but also how they are really the same."><meta property="og:type" content="article"><meta property="og:title" content="A Purely Functional Typed Approach to Trainable Models (Part 2)"><meta property="og:image" content="https://blog.jle.im/img/site_logo.jpg"><meta property="og:locale" content="en_US"><meta property="og:url" content="https://blog.jle.im/entry/purely-functional-typed-models-2.html"><meta name="twitter:card" content="summary"><meta name="twitter:creator:id" content="mstk"><link rel="author" href="https://plus.google.com/107705320197444500140"><link rel="alternate" type="application/rss+xml" title="in Code (RSS Feed)" href="http://feeds.feedburner.com/incodeblog"><link rel="canonical" href="https://blog.jle.im/entry/purely-functional-typed-models-2.html"><link href="https://blog.jle.im/favicon.ico" rel="shortcut icon"><link href="https://blog.jle.im/css/toast.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/font.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/main.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/page/entry.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/pygments.css" rel="stylesheet" type="text/css"><script type="text/javascript">var page_data = {};
var disqus_shortname='incode';
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-443711-8', 'jle.im');
ga('send', 'pageview');
</script><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5234d67a6b68dcd4"></script><script type="text/javascript" src="https://blog.jle.im/js/page/entry_toc.js"></script><script type="text/javascript" src="https://blog.jle.im/js/disqus_count.js"></script><script type="text/javascript" src="https://blog.jle.im/js/social.js"></script><script type="text/javascript" src="https://blog.jle.im/js/jquery/jquery.toc.js"></script><script type="text/javascript" src="https://blog.jle.im/purescript/entry.js"></script></head><body><div id="fb-root"><script>(function(d, s, id) {
 var js, fjs = d.getElementsByTagName(s)[0];
 if (d.getElementById(id)) return;
 js = d.createElement(s); js.id = id;
 js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=641852699171929";
 fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script></div><div id="header-container"><div id="navbar-container" class="tile"><nav id="navbar-content"><div class="nav-info"><h1 class="site-title"><a href="https://blog.jle.im/" class="nav-title">in Code</a></h1><span class="nav-author">Justin Le</span></div><ul class="nav-links"><li><a href="https://blog.jle.im/">home</a></li><li><a href="https://blog.jle.im/entries.html">archives</a></li><div class="clear"></div></ul></nav></div><div id="header-content"></div></div><div id="body-container" class="container"><div id="main-container" class="grid"><div class="entry-section unit span-grid" role="main"><article class="tile article"><header><h1 id="title">A Purely Functional Typed Approach to Trainable Models (Part 2)</h1><p class="entry-info">by <a class="author" href="https://blog.jle.im/">Justin Le</a><span class="info-separator"> &diams; </span><time datetime="2018-05-14T12:16:18Z" pubdate="" class="pubdate">Monday May 14, 2018</time></p><p><span class="source-info"><a class="source-link" href="https://github.com/mstksg/inCode/tree/master/copy/entries/functional-models-2.md">Source</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://github.com/mstksg/inCode/tree/gh-pages/entry/purely-functional-typed-models-2.md">Markdown</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://blog.jle.im/entry/purely-functional-typed-models-2.tex">LaTeX</a><span class="info-separator"> &diams; </span></span>Posted in <a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category" title="Functional, pure, non-strict, statically and strongly typed, natively
compiled...really just the king of great languages.">Haskell</a><span class="info-separator"> &diams; </span><a class="comment-link" href="#disqus_thread">Comments</a></p></header><hr><aside class="contents-container"><h5 id="contents-header">Contents</h5><div id="toc"></div></aside><div class="main-content copy-content"><p>Welcome back! We’re going to be jumping right back into describing a vision of a purely functional typed approach to writing trainable models using differentiable programming. If you’re just joining us, be sure to check out <a href="https://blog.jle.im/entry/purely-functional-typed-models-1.html">Part 1</a> first!</p>
<p>In the last post, we looked at models as “question and answer” systems. We described them as essentially being functions of type</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af%20%3A%20P%20%5Crightarrow%20%28A%20%5Crightarrow%20B%29%0A" alt="
f : P \rightarrow (A \rightarrow B)
" title="
f : P \rightarrow (A \rightarrow B)
" /><br /></p>
<p>Where, for <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f_p%28x%29%20%3D%20y" alt="f_p(x) = y" title="f_p(x) = y" />, you have a “question” <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?x%20%3A%20A" alt="x : A" title="x : A" /> and are looking for an “answer” <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?y%20%3A%20B" alt="y : B" title="y : B" />. Picking a <em>different</em> <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p%20%3A%20P" alt="p : P" title="p : P" /> will give a <em>different</em> <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A%20%5Crightarrow%20B" alt="A \rightarrow B" title="A \rightarrow B" /> function. We claimed that training a model was finding just the right <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?p" alt="p" title="p" /> to use with the model to yield the right <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?A%20%5Crightarrow%20B" alt="A \rightarrow B" title="A \rightarrow B" /> function that models your situation.</p>
<p>We then noted that if you have a set of <code>(a, b)</code> observations, and your function is differentiable, you can find the <em>gradient</em> of <code>p</code> with respect to the error of your model on each observation, which tells you how to nudge a given <code>p</code> in order to reduce how wrong your model is for that observation. By repeatedly making observations and taking those nudges, you can arrive at a suitable <code>p</code> to model any situation.</p>
<p>This is great if we consider a model as “question and answer”, but sometimes things don’t fit so cleanly. Today, we’re going to be looking at a whole different type of model (“time series” models) and see how they are different, but also how they are really the same.</p>
<p>For following along, the source code for the written code in this module is all available <a href="https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs">on github</a>.</p>
<h2 id="time-series-models">Time Series Models</h2>
<p>In the wild, many models are not simple “question and answer”, but rather represent a “time series”. As a generalization, we can talk about time series models as:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af_p%28x%2Ct%29%20%3D%20y%0A" alt="
f_p(x,t) = y
" title="
f_p(x,t) = y
" /><br /></p>
<p>Which says, given an input and a time, return an output based on both. The point of this is to let us have recurrent relationships, like for <a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive models</a> found in statistics:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BAR%7D_%7B%5Cphi_1%2C%20%5Cphi_2%2C%20%5Cldots%7D%28x%2Ct%29%0A%20%20%3D%20%5Cepsilon_t%20%2B%20%5Cphi_1%20%5Ctext%7BAR%7D_%7B%5Cphi_1%2C%20%5Cphi_2%2C%20%5Cldots%7D%28x%2C%20t-1%29%0A%20%20%2B%20%5Cphi_2%20%5Ctext%7BAR%7D_%7B%5Cphi_1%2C%20%5Cphi_2%2C%20%5Cldots%7D%28x%2C%20t-2%29%0A%20%20%2B%20%5Cldots%0A" alt="
\text{AR}_{\phi_1, \phi_2, \ldots}(x,t)
  = \epsilon_t + \phi_1 \text{AR}_{\phi_1, \phi_2, \ldots}(x, t-1)
  + \phi_2 \text{AR}_{\phi_1, \phi_2, \ldots}(x, t-2)
  + \ldots
" title="
\text{AR}_{\phi_1, \phi_2, \ldots}(x,t)
  = \epsilon_t + \phi_1 \text{AR}_{\phi_1, \phi_2, \ldots}(x, t-1)
  + \phi_2 \text{AR}_{\phi_1, \phi_2, \ldots}(x, t-2)
  + \ldots
" /><br /></p>
<p>However, this is a bad way of <em>implenting</em> models on time serieses, because nothing is stopping the result of a model from depending on a future value (the value at time <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?t%20%3D%203" alt="t = 3" title="t = 3" />, for instance, might depend explicitly only the value at time <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?t%20%3D%205" alt="t = 5" title="t = 5" />). Instead, we can imagine time series models as explicitly “stateful” models:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af_p%28x%2C%20s_%7B%5Ctext%7Bold%7D%7D%29%20%3D%20%28y%2C%20s_%7B%5Ctext%7Bnew%7D%7D%29%0A" alt="
f_p(x, s_{\text{old}}) = (y, s_{\text{new}})
" title="
f_p(x, s_{\text{old}}) = (y, s_{\text{new}})
" /><br /></p>
<p>These have type:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af%20%3A%20%28P%20%5Ctimes%20A%20%5Ctimes%20S%29%20%5Crightarrow%20%28B%20%5Ctimes%20S%29%0A" alt="
f : (P \times A \times S) \rightarrow (B \times S)
" title="
f : (P \times A \times S) \rightarrow (B \times S)
" /><br /></p>
<p>This makes it clear that the output of our model can only depend on current and <em>previously occurring</em> information, preserving causality.</p>
<h3 id="examples">Examples</h3>
<p>We can use this to represent an AR(2) model (<a href="https://en.wikipedia.org/wiki/Autoregressive_model">autoregressive model with degree 2</a>), which is a model whose output forecast is a linear regression on the <em>last two</em> most recent observed values. We can do this by setting the “input” to be the last observed value, and the “state” to be the second-to-last observed value:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0As_t%20%26%20%3D%20x_t%20%5C%5C%0Ay_t%20%26%20%3D%20c%20%2B%20%5Cphi_1%20s_t%20%2B%20%5Cphi_2%20s_%7Bt%20-%201%7D%0A%5Cend%7Baligned%7D%0A" alt="
\begin{aligned}
s_t &amp; = x_t \\
y_t &amp; = c + \phi_1 s_t + \phi_2 s_{t - 1}
\end{aligned}
" title="
\begin{aligned}
s_t &amp; = x_t \\
y_t &amp; = c + \phi_1 s_t + \phi_2 s_{t - 1}
\end{aligned}
" /><br /></p>
<p>Or, in our function form:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af_%7Bc%2C%20%5Cphi_1%2C%20%5Cphi_2%7D%28x%2C%20s%29%20%3D%20%28c%20%2B%20%5Cphi_1%20x%20%2B%20%5Cphi_2%20s%2C%20x%29%0A" alt="
f_{c, \phi_1, \phi_2}(x, s) = (c + \phi_1 x + \phi_2 s, x)
" title="
f_{c, \phi_1, \phi_2}(x, s) = (c + \phi_1 x + \phi_2 s, x)
" /><br /></p>
<p>There’s also the classic <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">fully-connected recurrent neural network layer</a>, whose output is a linear combination of the (logistic’d) previous output and the current input, plus a bias:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0As_t%20%26%20%3D%20%5Csigma%28y_t%29%20%5C%5C%0Ay_t%20%26%20%3D%20W_x%20%5Cmathbf%7Bx%7D_t%20%2B%20W_s%20%5Cmathbf%7Bs%7D_%7Bt-1%7D%20%2B%20%5Cmathbf%7Bb%7D%0A%5Cend%7Baligned%7D%0A" alt="
\begin{aligned}
s_t &amp; = \sigma(y_t) \\
y_t &amp; = W_x \mathbf{x}_t + W_s \mathbf{s}_{t-1} + \mathbf{b}
\end{aligned}
" title="
\begin{aligned}
s_t &amp; = \sigma(y_t) \\
y_t &amp; = W_x \mathbf{x}_t + W_s \mathbf{s}_{t-1} + \mathbf{b}
\end{aligned}
" /><br /></p>
<p>Or, in our function form:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Af_%7BW_x%2C%20W_s%2C%20%5Cmathbf%7Bb%7D%7D%28%5Cmathbf%7Bx%7D%2C%20%5Cmathbf%7Bs%7D%29%20%3D%0A%20%20%28%20W_x%20%5Cmathbf%7Bx%7D%20%2B%20W_s%20%5Cmathbf%7Bs%7D%20%2B%20%5Cmathbf%7Bb%7D%0A%20%20%2C%20%5Csigma%28W_x%20%5Cmathbf%7Bx%7D%20%2B%20W_s%20%5Cmathbf%7Bs%7D%20%2B%20%5Cmathbf%7Bb%7D%29%0A%20%20%29%0A" alt="
f_{W_x, W_s, \mathbf{b}}(\mathbf{x}, \mathbf{s}) =
  ( W_x \mathbf{x} + W_s \mathbf{s} + \mathbf{b}
  , \sigma(W_x \mathbf{x} + W_s \mathbf{s} + \mathbf{b})
  )
" title="
f_{W_x, W_s, \mathbf{b}}(\mathbf{x}, \mathbf{s}) =
  ( W_x \mathbf{x} + W_s \mathbf{s} + \mathbf{b}
  , \sigma(W_x \mathbf{x} + W_s \mathbf{s} + \mathbf{b})
  )
" /><br /></p>
<h3 id="the-connection">The connection</h3>
<p>This is nice and all, but these stateful models seem to be at odds with our previous picture of models.</p>
<ol type="1">
<li>They aren’t stated in the same way. They require specifying a state of some sort, and also a modified state</li>
<li>These can’t be <em>trained</em> in the same way (using stochastic gradient descent), and look like they require a different algorithm for training.</li>
</ol>
<p>However, because these are all <em>just functions</em>, we can really just manipulate them as normal functions and see that the two aren’t too different at all.</p>
<h2 id="functional-stateful-models">Functional Stateful Models</h2>
<p>Alright, so what does this mean, and how does it help us?</p>
<p>To help us see, let’s try implementing this in Haskell. Remember our previous <code>Model</code> type:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L49-L52</span></a>
<a class="sourceLine" id="cb1-2" data-line-number="2"></a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw">type</span> <span class="dt">Model</span> p a b <span class="fu">=</span> forall z<span class="fu">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span></a>
<a class="sourceLine" id="cb1-4" data-line-number="4">                <span class="ot">=&gt;</span> <span class="dt">BVar</span> z p</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">                <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">                <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b</a></code></pre></div>
<p>which represented a differentiable <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%20%3A%20%28P%20%5Ctimes%20A%29%20%5Crightarrow%20B" alt="f : (P \times A) \rightarrow B" title="f : (P \times A) \rightarrow B" />. We can directly translate this to a new <code>ModelS</code> type:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L154-L158</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="kw">type</span> <span class="dt">ModelS</span> p s a b <span class="fu">=</span> forall z<span class="fu">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">                   <span class="ot">=&gt;</span> <span class="dt">BVar</span> z p</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">                   <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</a>
<a class="sourceLine" id="cb2-6" data-line-number="6">                   <span class="ot">-&gt;</span> <span class="dt">BVar</span> z s</a>
<a class="sourceLine" id="cb2-7" data-line-number="7">                   <span class="ot">-&gt;</span> (<span class="dt">BVar</span> z b, <span class="dt">BVar</span> z s)</a></code></pre></div>
<p>which represents a differentiable <img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?f%20%3A%20%28P%20%5Ctimes%20A%20%5Ctimes%20S%29%20%5Crightarrow%20%28B%20%5Ctimes%20S%29" alt="f : (P \times A \times S) \rightarrow (B \times S)" title="f : (P \times A \times S) \rightarrow (B \times S)" />.</p>
<p>We can implement AR(2) as mentioned before by translating the math formula directly:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L160-L162</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"></a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="ot">ar2 ::</span> <span class="dt">ModelS</span> (<span class="dt">Double</span> <span class="fu">:&amp;</span> (<span class="dt">Double</span> <span class="fu">:&amp;</span> <span class="dt">Double</span>)) <span class="dt">Double</span> <span class="dt">Double</span> <span class="dt">Double</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">ar2 (c <span class="fu">:&amp;&amp;</span> (φ1 <span class="fu">:&amp;&amp;</span> φ2)) yLast yLastLast <span class="fu">=</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5">    ( c <span class="fu">+</span> φ1 <span class="fu">*</span> yLast <span class="fu">+</span> φ2 <span class="fu">*</span> yLastLast, yLast )</a></code></pre></div>
<p>Our implementation of a fully-connected recurrent neural network is a similar direct translation:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L164-L169</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2"></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">fcrnn</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</a>
<a class="sourceLine" id="cb4-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> ((<span class="dt">L</span> o i <span class="fu">:&amp;</span> <span class="dt">L</span> o o) <span class="fu">:&amp;</span> <span class="dt">R</span> o) (<span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</a>
<a class="sourceLine" id="cb4-6" data-line-number="6">fcrnn ((wX <span class="fu">:&amp;&amp;</span> wS) <span class="fu">:&amp;&amp;</span> b) x s <span class="fu">=</span> ( y, logistic y )</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">    y  <span class="fu">=</span> (wX <span class="fu">#&gt;</span> x) <span class="fu">+</span> (wS <span class="fu">#&gt;</span> s) <span class="fu">+</span> b</a></code></pre></div>
<p>Because we again have normal functions, we can write a similar stateful model composition function that combines both their parameters and their states:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L171-L180</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"></a>
<a class="sourceLine" id="cb5-3" data-line-number="3">(<span class="fu">&lt;*~*</span>)</a>
<a class="sourceLine" id="cb5-4" data-line-number="4"><span class="ot">  ::</span> (<span class="dt">Backprop</span> p, <span class="dt">Backprop</span> q, <span class="dt">Backprop</span> s, <span class="dt">Backprop</span> t)</a>
<a class="sourceLine" id="cb5-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span>  p        s       b c</a>
<a class="sourceLine" id="cb5-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span>       q        t  a b</a>
<a class="sourceLine" id="cb5-7" data-line-number="7">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> (p <span class="fu">:&amp;</span> q) (s <span class="fu">:&amp;</span> t) a c</a>
<a class="sourceLine" id="cb5-8" data-line-number="8">(f <span class="fu">&lt;*~*</span> g) (p <span class="fu">:&amp;&amp;</span> q) x (s <span class="fu">:&amp;&amp;</span> t) <span class="fu">=</span> (z, s&#39; <span class="fu">:&amp;&amp;</span> t&#39;)</a>
<a class="sourceLine" id="cb5-9" data-line-number="9">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb5-10" data-line-number="10">    (y, t&#39;) <span class="fu">=</span> g q x t</a>
<a class="sourceLine" id="cb5-11" data-line-number="11">    (z, s&#39;) <span class="fu">=</span> f p y s</a>
<a class="sourceLine" id="cb5-12" data-line-number="12"><span class="kw">infixr</span> <span class="dv">8</span> <span class="fu">&lt;*~*</span></a></code></pre></div>
<p>(Here we use our handy <code>(:&amp;&amp;)</code> pattern to construct a tuple, taking a <code>BVar z a</code> and a <code>BVar z b</code> and returning a <code>BVar z (a :&amp; b)</code>)</p>
<p>And maybe even a utility function to map a function on the result of a <code>ModelS</code>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L182-L186</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">mapS</a>
<a class="sourceLine" id="cb6-4" data-line-number="4"><span class="ot">    ::</span> (forall z<span class="fu">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span> <span class="ot">=&gt;</span> <span class="dt">BVar</span> z b <span class="ot">-&gt;</span> <span class="dt">BVar</span> z c)</a>
<a class="sourceLine" id="cb6-5" data-line-number="5">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s a b</a>
<a class="sourceLine" id="cb6-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s a c</a>
<a class="sourceLine" id="cb6-7" data-line-number="7">mapS f g p x <span class="fu">=</span> first f <span class="fu">.</span> g p x</a></code></pre></div>
<p>With this we can do some neat things like define a two-layer fully-connected recurrent neural network.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb7-1" data-line-number="1">ghci<span class="fu">&gt;</span> <span class="kw">let</span><span class="ot"> twoLayerRNN ::</span> <span class="dt">ModelS</span> _ _ (<span class="dt">R</span> <span class="dv">20</span>) (<span class="dt">R</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">          twoLayerRNN <span class="fu">=</span> fcrnn <span class="fu">@</span><span class="dv">10</span> <span class="fu">&lt;*~*</span> mapS logistic fcrnn</a></code></pre></div>
<p>(Again using type application syntax with <code>@10</code> to specify our hidden layer size, and the type wildcard syntax <code>_</code> to let the compiler fill in the parameter and state type for us)</p>
<p>Hey, maybe even a three-layer one:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb8-1" data-line-number="1">ghci<span class="fu">&gt;</span> <span class="kw">let</span><span class="ot"> threeLayers ::</span> <span class="dt">ModelS</span> _ _ (<span class="dt">R</span> <span class="dv">40</span>) (<span class="dt">R</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb8-2" data-line-number="2">          threeLayers <span class="fu">=</span> fcrnn <span class="fu">@</span><span class="dv">10</span></a>
<a class="sourceLine" id="cb8-3" data-line-number="3">                   <span class="fu">&lt;*~*</span> mapS logistic (fcrnn <span class="fu">@</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">                   <span class="fu">&lt;*~*</span> mapS logistic fcrnn</a></code></pre></div>
<h3 id="let-there-be-state">Let there be State</h3>
<p>Because these are all just normal functions, we can manipulate them just like any other function using higher order functions.</p>
<p>For example, we can “upgrade” any non-stateful function to a stateful one, just by returning a new normal function:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L188-L190</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"></a>
<a class="sourceLine" id="cb9-3" data-line-number="3"><span class="ot">toS ::</span> <span class="dt">Model</span>  p   a b</a>
<a class="sourceLine" id="cb9-4" data-line-number="4">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s a b</a>
<a class="sourceLine" id="cb9-5" data-line-number="5">toS f p x s <span class="fu">=</span> (f p x, s)</a></code></pre></div>
<p>This means we can make a hybrid “recurrent” and “non-recurrent” neural network, by making <code>feedForwardLog'</code> a model with some dummy state (like <code>()</code> perhaps), and re-using <code>(&lt;*~*)</code>.</p>
<p>But we can also be creative with our combinators, as well, and write one to compose a stateless model with a stateful one:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L192-L198</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2"></a>
<a class="sourceLine" id="cb10-3" data-line-number="3">(<span class="fu">&lt;*~</span>)</a>
<a class="sourceLine" id="cb10-4" data-line-number="4"><span class="ot">  ::</span> (<span class="dt">Backprop</span> p, <span class="dt">Backprop</span> q)</a>
<a class="sourceLine" id="cb10-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">Model</span>   p         b c</a>
<a class="sourceLine" id="cb10-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span>       q  s a b</a>
<a class="sourceLine" id="cb10-7" data-line-number="7">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> (p <span class="fu">:&amp;</span> q) s a c</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">(f <span class="fu">&lt;*~</span> g) (p <span class="fu">:&amp;&amp;</span> q) x <span class="fu">=</span> first (f p) <span class="fu">.</span> g q x</a>
<a class="sourceLine" id="cb10-9" data-line-number="9"><span class="kw">infixr</span> <span class="dv">8</span> <span class="fu">&lt;*~</span></a></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb11-1" data-line-number="1">ghci<span class="fu">&gt;</span> <span class="kw">let</span><span class="ot"> hybrid ::</span> <span class="dt">ModelS</span> _ _ (<span class="dt">R</span> <span class="dv">40</span>) (<span class="dt">R</span> <span class="dv">10</span>)</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">          hybrid <span class="fu">=</span> feedForwardLog&#39; <span class="fu">@</span><span class="dv">20</span></a>
<a class="sourceLine" id="cb11-3" data-line-number="3">              <span class="fu">&lt;*~</span>  mapS logistic (fcrnn <span class="fu">@</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb11-4" data-line-number="4">              <span class="fu">&lt;*~*</span> mapS logistic fcrnn</a></code></pre></div>
<p>Everything is just your simple run-of-the-mill function composition and higher order functions that Haskellers use every day, so there are many ways to do these things — just like there are many ways to manipulate normal functions.</p>
<h2 id="unrolling-in-the-deep-learning">Unrolling in the Deep (Learning)</h2>
<p>There’s something neat we can do with stateful functions — we can “<a href="https://machinelearningmastery.com/rnn-unrolling/">unroll</a>” them by explicitly propagating their state through several inputs.</p>
<p>This is illustrated very well by <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">Christopher Olah</a>, who made a diagram that illustrates the idea very well:</p>
<figure>
<img src="/img/entries/functional-models/RNN-general.png" title="Unrolled RNN" alt="Christopher Olah’s RNN Unrolling Diagram" /><figcaption>Christopher Olah’s RNN Unrolling Diagram</figcaption>
</figure>
<p>If we look at each one of those individual boxes, they all have two inputs (normal input, and previous state) and two outputs (normal output, new state).</p>
<p>“Unrolling” a stateful model means taking a model that takes in an <code>X</code> and producing a <code>Y</code> and turning it into a model that takes an <code>[X]</code> and produces a <code>[Y]</code>, by feeding it each of the <code>X</code>s one after the other, propagating the state, and collecting all of the <code>Y</code> responses.</p>
<p>The “type” of this sounds like:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="ot">unroll ::</span> <span class="dt">Model</span> p s a b <span class="ot">-&gt;</span> <span class="dt">Model</span> p s [a] [b]</a></code></pre></div>
<p>In writing this out as a type, we also note that the <code>p</code> parameter type is the same, and the <code>s</code> state type is the same. (Aren’t types nice? They force you to have to think about subtle things like this) If you’re familiar with category theory, this looks a little bit like a sort of “fmap” under a <code>Model p s</code> category – it takes a (stateful and backpropagatable) <code>a -&gt; b</code> and turns it into an <code>[a] -&gt; [b]</code>.</p>
<p>Olah’s post suggests that this is some sort of <code>mapAccum</code>, in functional programming parlance. And, surely enough, we can actually write this as a <code>mapAccumL</code>.</p>
<p><code>mapAccumL</code> is sort of like a combination of a <code>foldl</code> and a <code>map</code>:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb13-1" data-line-number="1">mapAccumL</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="ot">    ::</span> (a <span class="ot">-&gt;</span> b <span class="ot">-&gt;</span> (a, c))</a>
<a class="sourceLine" id="cb13-3" data-line-number="3">    <span class="ot">-&gt;</span> a</a>
<a class="sourceLine" id="cb13-4" data-line-number="4">    <span class="ot">-&gt;</span> [b]</a>
<a class="sourceLine" id="cb13-5" data-line-number="5">    <span class="ot">-&gt;</span> (a, [c])</a></code></pre></div>
<p>Compare to <code>foldl</code>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb14-1" data-line-number="1">foldl</a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="ot">    ::</span> (a <span class="ot">-&gt;</span> b <span class="ot">-&gt;</span> a)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">    <span class="ot">-&gt;</span> a</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">    <span class="ot">-&gt;</span> [b]</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">    <span class="ot">-&gt;</span> a</a></code></pre></div>
<p>You can see that <code>mapAccumL</code> is just <code>foldl</code>, except the folding function emits an extra <code>c</code> for every item, so <code>mapAccumL</code> can return a new <code>[c]</code> with all of the emitted <code>c</code>s.</p>
<p>The <em>backprop</em> library has a “lifted” <code>mapAccumL</code> in in the <em><a href="http://hackage.haskell.org/package/backprop/docs/Prelude-Backprop.html">Prelude.Backprop</a></em> module that we can use:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb15-1" data-line-number="1">Prelude.Backprop.mapAccumL</a>
<a class="sourceLine" id="cb15-2" data-line-number="2"><span class="ot">    ::</span> (<span class="dt">BVar</span> z a <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b <span class="ot">-&gt;</span> (<span class="dt">BVar</span> z a, <span class="dt">BVar</span> z c))</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z [b]</a>
<a class="sourceLine" id="cb15-5" data-line-number="5">    <span class="ot">-&gt;</span> (<span class="dt">BVar</span> z a, <span class="dt">BVar</span> z [c])</a></code></pre></div>
<p>It is lifted to work with <code>BVar</code>s of the items instead of directly on the items. With that, we can write <code>unroll</code>, which is just a thin wrapper over <code>mapAccumL</code>:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L200-L208</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2"></a>
<a class="sourceLine" id="cb16-3" data-line-number="3">unroll</a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">Backprop</span> a, <span class="dt">Backprop</span> b)</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> p s  a   b</a>
<a class="sourceLine" id="cb16-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s [a] [b]</a>
<a class="sourceLine" id="cb16-7" data-line-number="7">unroll f p xs s0 <span class="fu">=</span> swap <span class="fu">$</span> B.mapAccumL f&#39; s0 xs</a>
<a class="sourceLine" id="cb16-8" data-line-number="8">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb16-9" data-line-number="9">    <span class="co">-- we have to re-arrange the order of arguments and tuple a bit to</span></a>
<a class="sourceLine" id="cb16-10" data-line-number="10">    <span class="co">-- match what `mapAccumL` expects</span></a>
<a class="sourceLine" id="cb16-11" data-line-number="11">    f&#39; s x <span class="fu">=</span> swap (f p x s)</a></code></pre></div>
<p>This reveals that <code>unroll</code> from the machine learning is really <em>just</em> <code>mapAccumL</code> from functional programming.</p>
<p>We can also tweak <code>unroll</code>’s result a bit to get a version of <code>unroll</code> that shows only the “final” result. All we do is <code>mapS</code> <code>last . sequenceVar :: BVar s [a] -&gt; BVar a</code>, which gets the last item in a <code>BVar</code> of a sequence.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L210-L214</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2"></a>
<a class="sourceLine" id="cb17-3" data-line-number="3">unrollLast</a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">Backprop</span> a, <span class="dt">Backprop</span> b)</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> p s  a  b</a>
<a class="sourceLine" id="cb17-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s [a] b</a>
<a class="sourceLine" id="cb17-7" data-line-number="7">unrollLast f <span class="fu">=</span> mapS (last <span class="fu">.</span> sequenceVar) (unroll f)</a></code></pre></div>
<p>Alternatively, we can also recognize that <code>unrollLast</code> is really just an awkward left fold (<code>foldl</code>) in disguise:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L216-L222</span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2"></a>
<a class="sourceLine" id="cb18-3" data-line-number="3">unrollLast&#39;</a>
<a class="sourceLine" id="cb18-4" data-line-number="4"><span class="ot">    ::</span> <span class="dt">Backprop</span> a</a>
<a class="sourceLine" id="cb18-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> p s  a  b</a>
<a class="sourceLine" id="cb18-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s [a] b</a>
<a class="sourceLine" id="cb18-7" data-line-number="7">unrollLast&#39; f p xs s0 <span class="fu">=</span> foldl&#39; go (undefined, s0) (sequenceVar xs)</a>
<a class="sourceLine" id="cb18-8" data-line-number="8">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb18-9" data-line-number="9">    go (_, s) x <span class="fu">=</span> f p x s</a></code></pre></div>
<p>To see how this applies to our <code>threeLayer</code>:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="ot">threeLayers            ::</span> <span class="dt">ModelS</span> _ _ (<span class="dt">R</span> <span class="dv">40</span>) (<span class="dt">R</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb19-2" data-line-number="2">unroll<span class="ot">     threeLayers ::</span> <span class="dt">ModelS</span> _ _ [<span class="dt">R</span> <span class="dv">40</span>] [<span class="dt">R</span> <span class="dv">5</span>]</a>
<a class="sourceLine" id="cb19-3" data-line-number="3">unrollLast<span class="ot"> threeLayers ::</span> <span class="dt">ModelS</span> _ _ [<span class="dt">R</span> <span class="dv">40</span>] (<span class="dt">R</span> <span class="dv">5</span>)</a></code></pre></div>
<p>Nice that we can trace the evolution of the types within our langage!</p>
<h3 id="state-be-gone">State-be-gone</h3>
<p>Did you enjoy the detour through stateful time series models?</p>
<p>Good — because the whole point of it was to talk about how we can <em>get rid of state</em> and bring us back to our original models!</p>
<p>You knew this day had to come, because all of our methods for “training” these models and learn these parameters involves non-stateful models. Let’s see now how we can turn our functional stateful models into functional non-stateful models!</p>
<p>One way is to <em>fix the initial state and throw away the resulting one</em>. This is very common in machine learning contexts, where many people simply fix the initial state to be a zero vector.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L225-L235</span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2"></a>
<a class="sourceLine" id="cb20-3" data-line-number="3">fixState</a>
<a class="sourceLine" id="cb20-4" data-line-number="4"><span class="ot">    ::</span> s</a>
<a class="sourceLine" id="cb20-5" data-line-number="5">    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p s a b</a>
<a class="sourceLine" id="cb20-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">Model</span>  p   a b</a>
<a class="sourceLine" id="cb20-7" data-line-number="7">fixState s0 f p x <span class="fu">=</span> fst <span class="fu">$</span> f p x (auto s0)</a>
<a class="sourceLine" id="cb20-8" data-line-number="8"></a>
<a class="sourceLine" id="cb20-9" data-line-number="9">zeroState</a>
<a class="sourceLine" id="cb20-10" data-line-number="10"><span class="ot">    ::</span> <span class="dt">Num</span> s</a>
<a class="sourceLine" id="cb20-11" data-line-number="11">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> p s a b</a>
<a class="sourceLine" id="cb20-12" data-line-number="12">    <span class="ot">-&gt;</span> <span class="dt">Model</span>  p   a b</a>
<a class="sourceLine" id="cb20-13" data-line-number="13">zeroState <span class="fu">=</span> fixState <span class="dv">0</span></a></code></pre></div>
<p>We use <code>auto :: a -&gt; BVar z a</code> again to introduce a <code>BVar</code> of our initial state, but to indicate that we don’t expect to track its gradient. <code>zeroState</code> is a nice utility combinator for a common pattern.</p>
<p>Another way is to <em>treat the initial state as a trainable parameter</em> (and also throw away the final state). This is not done as often, but is still common enough to be mentioned often. And, it’s just as straightforward!</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L237-L241</span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2"></a>
<a class="sourceLine" id="cb21-3" data-line-number="3">trainState</a>
<a class="sourceLine" id="cb21-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">Backprop</span> p, <span class="dt">Backprop</span> s)</a>
<a class="sourceLine" id="cb21-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span>  p    s  a b</a>
<a class="sourceLine" id="cb21-6" data-line-number="6">    <span class="ot">-&gt;</span> <span class="dt">Model</span>  (p <span class="fu">:&amp;</span> s) a b</a>
<a class="sourceLine" id="cb21-7" data-line-number="7">trainState f (p <span class="fu">:&amp;&amp;</span> s) x <span class="fu">=</span> fst <span class="fu">$</span> f p x s</a></code></pre></div>
<p><code>trainState</code> will take a model with trainable parameter <code>p</code> and state <code>s</code>, and turn it into a model with trainable parameter <code>p :&amp; s</code>, where the <code>s</code> is the (trainable) initial state.</p>
<p>We can now <em>train</em> our recurrent/stateful models, by <strong>unrolling and de-stating</strong>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="ot">threeLayers                        ::</span> <span class="dt">ModelS</span> _ _ (<span class="dt">R</span> <span class="dv">40</span>) (<span class="dt">R</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb22-2" data-line-number="2">unrollLast<span class="ot"> threeLayers             ::</span> <span class="dt">ModelS</span> _ _ [<span class="dt">R</span> <span class="dv">40</span>] (<span class="dt">R</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb22-3" data-line-number="3">zeroState (unrollLast threeLayers)<span class="ot"> ::</span> <span class="dt">Model</span>  _   [<span class="dt">R</span> <span class="dv">40</span>] (<span class="dt">R</span> <span class="dv">5</span>)</a></code></pre></div>
<p><code>zeroState (unrollLast threeLayers)</code> is now a normal stateless (and trainable) model. It takes a list of inputs <code>R 40</code>s and produces the “final output” <code>R 5</code>. We can now train this by feeding it with <code>([R 40], R 5)</code> pairs: give a history and an expected next output.</p>
<p>It’s again nice here how we can track the evolution of the types of out model’s inputs and outputs within the language. Unrolling and zeroing is a non-trivial interaction, so the ability to have the language and compiler track the resulting shapes of our models is a huge advantage.</p>
<h3 id="the-unreasonably-effective">The Unreasonably Effective</h3>
<p>Let’s see if we can train a two-layer fully connected neural network with 30 hidden units, where the first layer is fully recurrent, to learn how to model a sine wave:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="co">-- sine signal with period 25</span></a>
<a class="sourceLine" id="cb23-2" data-line-number="2">ghci<span class="fu">&gt;</span> series <span class="fu">=</span> [ sin (<span class="dv">2</span> <span class="fu">*</span> pi <span class="fu">*</span> t <span class="fu">/</span> <span class="dv">25</span>) <span class="fu">|</span> t <span class="ot">&lt;-</span> [<span class="dv">0</span><span class="fu">..</span>]              ]</a>
<a class="sourceLine" id="cb23-3" data-line-number="3"></a>
<a class="sourceLine" id="cb23-4" data-line-number="4"><span class="co">-- chunks of runs and &quot;next results&quot;</span></a>
<a class="sourceLine" id="cb23-5" data-line-number="5">ghci<span class="fu">&gt;</span> samps  <span class="fu">=</span> [ (init c, last c)      <span class="fu">|</span> c <span class="ot">&lt;-</span> chunksOf <span class="dv">19</span> series ]</a>
<a class="sourceLine" id="cb23-6" data-line-number="6"></a>
<a class="sourceLine" id="cb23-7" data-line-number="7"><span class="co">-- first layer is RNN, second layer is normal ANN, 30 hidden units</span></a>
<a class="sourceLine" id="cb23-8" data-line-number="8">ghci<span class="fu">&gt;</span> <span class="kw">let</span><span class="ot"> rnn ::</span> <span class="dt">ModelS</span> _ _ (<span class="dt">R</span> <span class="dv">1</span>) (<span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb23-9" data-line-number="9">          rnn <span class="fu">=</span> feedForward <span class="fu">@</span><span class="dv">30</span> <span class="fu">&lt;*~</span> mapS logistic fcrnn</a>
<a class="sourceLine" id="cb23-10" data-line-number="10"></a>
<a class="sourceLine" id="cb23-11" data-line-number="11">ghci<span class="fu">&gt;</span> trained <span class="ot">&lt;-</span> trainModelIO (zeroState (unrollLast rnn)) <span class="fu">$</span> take <span class="dv">10000</span> samps</a></code></pre></div>
<p>Trained! <code>trained</code> is now the weight and bias matrices and vectors that will simulate a sine wave of period 25.</p>
<p>We can run this model iteratively upon itself to test it; if we plot the results, we can visually inspect it to see if it has learned things properly.</p>
<p>Let’s define some helper functions to test our model. First, a function <code>prime</code> that takes a stateful model and gives a “warmed-up” state by running it over a list of inputs. This serves to essentially initialize the memory of the model.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L243-L250</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2"></a>
<a class="sourceLine" id="cb24-3" data-line-number="3">prime</a>
<a class="sourceLine" id="cb24-4" data-line-number="4"><span class="ot">    ::</span> <span class="dt">Foldable</span> t</a>
<a class="sourceLine" id="cb24-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> p s a b     <span class="co">-- ^ model</span></a>
<a class="sourceLine" id="cb24-6" data-line-number="6">    <span class="ot">-&gt;</span> p                  <span class="co">-- ^ parameterization</span></a>
<a class="sourceLine" id="cb24-7" data-line-number="7">    <span class="ot">-&gt;</span> s                  <span class="co">-- ^ initial state</span></a>
<a class="sourceLine" id="cb24-8" data-line-number="8">    <span class="ot">-&gt;</span> t a                <span class="co">-- ^ priming input</span></a>
<a class="sourceLine" id="cb24-9" data-line-number="9">    <span class="ot">-&gt;</span> s                  <span class="co">-- ^ primed state</span></a>
<a class="sourceLine" id="cb24-10" data-line-number="10">prime f p <span class="fu">=</span> foldl&#39; <span class="fu">$</span> evalBP2 (\s x <span class="ot">-&gt;</span> snd <span class="fu">$</span> f (auto p) x s)</a></code></pre></div>
<p>Then a function <code>feedback</code> that iterates a stateful model over and over again by feeding its previous output as its next input:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L252-L264</span></a>
<a class="sourceLine" id="cb25-2" data-line-number="2"></a>
<a class="sourceLine" id="cb25-3" data-line-number="3">feedback</a>
<a class="sourceLine" id="cb25-4" data-line-number="4"><span class="ot">    ::</span> (<span class="dt">Backprop</span> a, <span class="dt">Backprop</span> s)</a>
<a class="sourceLine" id="cb25-5" data-line-number="5">    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> p s a a     <span class="co">-- ^ model</span></a>
<a class="sourceLine" id="cb25-6" data-line-number="6">    <span class="ot">-&gt;</span> p                  <span class="co">-- ^ parameterization</span></a>
<a class="sourceLine" id="cb25-7" data-line-number="7">    <span class="ot">-&gt;</span> s                  <span class="co">-- ^ initial state</span></a>
<a class="sourceLine" id="cb25-8" data-line-number="8">    <span class="ot">-&gt;</span> a                  <span class="co">-- ^ initial input</span></a>
<a class="sourceLine" id="cb25-9" data-line-number="9">    <span class="ot">-&gt;</span> [a]                <span class="co">-- ^ inifinite feedback loop</span></a>
<a class="sourceLine" id="cb25-10" data-line-number="10">feedback f p s0 x0 <span class="fu">=</span> unfoldr go (x0, s0)</a>
<a class="sourceLine" id="cb25-11" data-line-number="11">  <span class="kw">where</span></a>
<a class="sourceLine" id="cb25-12" data-line-number="12">    go (x, s) <span class="fu">=</span> <span class="dt">Just</span> (x, (y, s&#39;))</a>
<a class="sourceLine" id="cb25-13" data-line-number="13">      <span class="kw">where</span></a>
<a class="sourceLine" id="cb25-14" data-line-number="14">        <span class="co">-- &#39;T2&#39; tuples up a pair of &#39;BVar&#39;s into a &#39;BVar&#39; of a tuple</span></a>
<a class="sourceLine" id="cb25-15" data-line-number="15">        (y, s&#39;) <span class="fu">=</span> evalBP (uncurry <span class="dt">T2</span> <span class="fu">.</span> f (auto p) (auto x)) s</a></code></pre></div>
<p>Now let’s prime our trained model over the first 19 items in our sine wave and start it running in feedback mode on the 20th item!</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb26-1" data-line-number="1">ghci<span class="fu">&gt;</span> <span class="kw">let</span> primed <span class="fu">=</span> prime    rnn trained <span class="dv">0</span>      (take <span class="dv">19</span> series)</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">ghci<span class="fu">&gt;</span> <span class="kw">let</span> output <span class="fu">=</span> feedback rnn trained primed (series <span class="fu">!!</span> <span class="dv">19</span>)</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">ghci<span class="fu">&gt;</span> mapM_ print <span class="fu">$</span> take <span class="dv">200</span> output</a>
<a class="sourceLine" id="cb26-4" data-line-number="4">(<span class="fu">-</span><span class="fl">0.9980267284282716</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">(<span class="fu">-</span><span class="fl">0.9530599469923343</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-6" data-line-number="6">(<span class="fu">-</span><span class="fl">0.855333250123637</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-7" data-line-number="7">(<span class="fu">-</span><span class="fl">0.7138776465246676</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-8" data-line-number="8">(<span class="fu">-</span><span class="fl">0.5359655931506458</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb26-9" data-line-number="9"><span class="co">-- ...</span></a></code></pre></div>
<p>Plotting the result against the “actual” sine wave of period 25, we see that it approximates the process decently well, with a consistent period (that is slightly slower than the reference period):</p>
<figure>
<img src="/img/entries/functional-models/rnnsin.png" title="FCRNN Sine Wave" alt="FCRNN Sine Wave" /><figcaption>FCRNN Sine Wave</figcaption>
</figure>
<p>For kicks, let’s see if we can do any better with the simpler AR(2) model from before. Applying all we just used to <code>ar2</code>, we see:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="ot">ar2                        ::</span> <span class="dt">ModelS</span> _ _  <span class="dt">Double</span>  <span class="dt">Double</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2">unrollLast<span class="ot"> ar2             ::</span> <span class="dt">ModelS</span> _ _ [<span class="dt">Double</span>] <span class="dt">Double</span></a>
<a class="sourceLine" id="cb27-3" data-line-number="3">zeroState (unrollLast ar2)<span class="ot"> ::</span> <span class="dt">Model</span>  _   [<span class="dt">Double</span>] <span class="dt">Double</span></a></code></pre></div>
<p><code>zeroState (unrollLast ar2)</code> is now a trainable stateless model. Will it model a sine wave?</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb28-1" data-line-number="1">ghci<span class="fu">&gt;</span> trained <span class="ot">&lt;-</span> trainModelIO (zeroState (unrollLast ar2)) <span class="fu">$</span> take <span class="dv">10000</span> samps</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">ghci<span class="fu">&gt;</span> <span class="kw">let</span> primed <span class="fu">=</span> prime    rnn trained <span class="dv">0</span>      (take <span class="dv">19</span> series)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3">ghci<span class="fu">&gt;</span> <span class="kw">let</span> output <span class="fu">=</span> feedback rnn trained primed (series <span class="fu">!!</span> <span class="dv">19</span>)</a>
<a class="sourceLine" id="cb28-4" data-line-number="4">ghci<span class="fu">&gt;</span> mapM_ print <span class="fu">$</span> take <span class="dv">200</span> output</a>
<a class="sourceLine" id="cb28-5" data-line-number="5">(<span class="fu">-</span><span class="fl">0.9980267284282716</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb28-6" data-line-number="6">(<span class="fu">-</span><span class="fl">0.9530599469923343</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb28-7" data-line-number="7">(<span class="fu">-</span><span class="fl">0.855333250123637</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb28-8" data-line-number="8">(<span class="fu">-</span><span class="fl">0.7138776465246676</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb28-9" data-line-number="9">(<span class="fu">-</span><span class="fl">0.5359655931506458</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb28-10" data-line-number="10"><span class="co">-- ...</span></a></code></pre></div>
<p>We can plot the result and see that it more or less perfectly models the sine wave of period 25:</p>
<figure>
<img src="/img/entries/functional-models/ar2sin.png" title="AR Sine Wave" alt="AR(2) Sine Wave" /><figcaption>AR(2) Sine Wave</figcaption>
</figure>
<p>You can’t even visually see the difference!</p>
<p>We can peek inside the parameterization of our learned AR(2):</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb29-1" data-line-number="1">ghci<span class="fu">&gt;</span> trained</a>
<a class="sourceLine" id="cb29-2" data-line-number="2"><span class="fu">-</span><span class="fl">2.4013298985824788e-12</span> <span class="fu">:&amp;</span> (<span class="fl">1.937166322256747</span> <span class="fu">:&amp;</span> <span class="fu">-</span><span class="fl">0.9999999999997953</span>)</a>
<a class="sourceLine" id="cb29-3" data-line-number="3"><span class="co">-- approximately</span></a>
<a class="sourceLine" id="cb29-4" data-line-number="4"><span class="fl">0.0000</span> <span class="fu">:&amp;</span> (<span class="fl">1.9372</span> <span class="fu">:&amp;</span> <span class="fu">-</span><span class="fl">1.0000</span>)</a></code></pre></div>
<p>Meaning that the gradient descent has concluded that our AR(2) model is:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/png.latex?%0Ay_t%20%3D%200%20%2B%201.9372%20y_%7Bt%20-%201%7D%20-%20y_%7Bt%20-%202%7D%0A" alt="
y_t = 0 + 1.9372 y_{t - 1} - y_{t - 2}
" title="
y_t = 0 + 1.9372 y_{t - 1} - y_{t - 2}
" /><br /></p>
<p>The power of math!</p>
<p>In this toy situation, the AR(2) appears to do much better than our RNN model, but we have to give the RNN a break — all of the information has to be “squished” into essentially 30 bits, which might impact the model’s accuracy.</p>
<h2 id="functions-all-the-way-down">Functions all the way down</h2>
<p>Again, it is very easy to look at something like</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode haskell"><code class="sourceCode haskell"><a class="sourceLine" id="cb30-1" data-line-number="1">feedForward <span class="fu">@</span><span class="dv">10</span> <span class="fu">&lt;*~</span> mapS logistic fcrnn</a></code></pre></div>
<p>and write it off as some abstract API of opaque data types. Some sort of object keeps track of state, and the object has some nice abstracted interface…right?</p>
<p>But, nope, again, it is all just normal functions that we wrote using normal function composition. We define our model as a <em>function</em>, and the backprop library turns that function into a trainable model.</p>
<h3 id="what-makes-it-tick">What Makes It Tick</h3>
<p>Let’s again revisit the four things I mentioned that are essential to making this all work at the end of the last post, but update it with new observations that we made in this post:</p>
<ol type="1">
<li><p><em>Functional programming</em> is the paradigm that allowed us to treat everything as normal functions, so that our combinators are all just normal higher-order functions.</p>
<p>Our stateful models can also be combined and reshaped seamlessly in arbitrary ways, just like our non-stateful ones. And the fact that they are both normal functions means that they are built on the same underlying mechanic.</p>
<p>We can <em>write</em> combinators like <code>(&lt;*~*)</code> and <code>mapS</code>, but they are never <em>necessary</em>. They are always just <em>convenient</em>. But by writing such combinators, we open our mind to different ways that we can construct new models by simply transforming old ones.</p>
<p>The revelation that an unrolled model was simply a combinator application came about by simply looking at the types and applying a model to a simple higher order function <code>mapAccumL</code> and <code>foldl</code>, which was <em>already written for us</em>. We were able to use <em>common functional programming tools</em> that are provided in standard libraries. This is only possible because our models are themselves functions in the same shape that those common tools already are built to work on.</p>
<p>In addition, functional programming forces us to have <em>first-class state</em>. The “state” in our stateful models wasn’t a property of the runtime system — they were things we explicitly defined and carried. This allows us to write combinators that <em>manipulate how state works</em>. We can transform a function’s state arbitrarily because the function’s state is always something we can explicitly manipulate.</p></li>
<li><p><em>Differentiable</em> programs — again, made more powerful through how well it integrates with functional programming techniques.</p></li>
<li><p><em>Purely</em> functional programming. One might have thought that writing “recurrent” or “stateful” models were something that imperative models excelled in, but we see now that in a functional setting, forcing ourselves to use explicit state allows us to manipulate state and state manipulation as a first-class citizen of our language, instead of something built-in and implicit and rigid.</p></li>
<li><p>A <em>strong expressive static type system</em> ties all of this together and makes it possible to work in.</p>
<p>This forces us to be aware of what parameters we have, how they combine, etc.; this is what makes combinators like <code>recurrent</code> and <code>unroll</code> and <code>zeroState</code> reasonable: the <em>compiler</em> is able to trace how we move around our parameter and state, so that we don’t have to. It lets us ask <em>the compiler</em> questions like “what is the state type, now?” if we needed, or “what is the parameter type now?”.</p>
<p>We sometimes even gained insight simply from thinking, in advance, what the types of our combinators were. We had to make conscious decisions when writing the type of <code>unroll</code> and <code>zeroState</code>. And, if we can phrase our combinators in terms of our types, the compiler will often be able to write our entire program for us — something only possible for statically typed languages.</p></li>
</ol>
<p>In the <a href="https://blog.jle.im/entry/purely-functional-typed-models-3.html">next and final post</a>, we’ll wrap this up by peeking into the wonderful world of functional combinators and look at powerful ones that allow us to unify many different model types as really just different combinator applications of the same thing. I’ll also talk about what I think are essential in building a usable framework for working with this in practice.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If you recognized our original stateless model type as <code>a -&gt; Reader p b</code>, then you might have also recognized that this is the Haskell idiom <code>a -&gt; StateT s (Reader p) b</code> (or <code>Kleisli (StateT s (Reader p)) a b</code>), which represents the notion of a “function from <code>a</code> to <code>b</code> with environment <code>p</code>, that takes and returns a modified version of some ‘state’ <code>s</code>”.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>In truth, <code>mapAccumL</code> can work with any <code>Traversable</code> container, so we really can <code>unroll</code> over any <code>Traversable</code> container and not just lists. One of my favorite is the sized vectors from the <a href="http://hackage.haskell.org/package/vector-sized">vector-sized</a> library, since they can enforce that the network always gets unrolled over the same number of items.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</section></div><footer><hr><div class="copy-content"><p>You can reach me via email at <a href="mailto:justin@jle.im">justin@jle.im</a>, or at twitter at <span class="citation" data-cites="mstk">[@mstk]</span><a href="https://twitter.com/mstk">twitter</a>! This post and all others are published under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/">CC-BY-NC-ND 3.0</a> license. Corrections and edits via pull request are welcome and encouraged at [the source repository][rep].</p>
<p>If you would like to donate, I am currently accepting bitcoin donations at <em><a href="bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU">3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU</a></em>!</p></div><div class="clear"></div><ul class="entry-series"><li><div>This entry is a part of a series called <b>&quot;Functional Models&quot;</b>.  Find the rest of the entries in this series at its <a href="https://blog.jle.im/entries/series/+functional-models.html" class="tag-a-series" title="+Functional Models"> series history</a>.</div></li></ul><ul class="tag-list"><li><a href="https://blog.jle.im/entries/tagged/backprop.html" class="tag-a-tag">#backprop</a></li><li><a href="https://blog.jle.im/entries/tagged/deep-learning.html" class="tag-a-tag">#deep learning</a></li><li><a href="https://blog.jle.im/entries/tagged/differentiable-programming.html" class="tag-a-tag">#differentiable programming</a></li><li><a href="https://blog.jle.im/entries/tagged/machine-learning.html" class="tag-a-tag">#machine learning</a></li><li><a href="https://blog.jle.im/entries/tagged/modeling.html" class="tag-a-tag">#modeling</a></li><li><a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category">@HASKELL</a></li><li><a href="https://blog.jle.im/entries/series/+functional-models.html" class="tag-a-series">+Functional Models</a></li></ul><aside class="social-buttons"><div class="addthis_toolbox addthis_default_style addthis-buttons"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a><a class="addthis_button_tweet"></a><a class="addthis_button_google_plusone" g:plusone:size="medium"></a><a class="addthis_counter addthis_pill_style"></a></div><div class="custom-social-buttons"><div class="custom-social-button"><a href="https://www.reddit.com/submit" onclick="window.location = &#39;https://www.reddit.com/submit?url=&#39;+ encodeURIComponent(window.location); return false"><img src="https://www.reddit.com/static/spreddit7.gif" alt="submit to reddit"></a></div></div></aside><nav class="next-prev-links"><ul><li class="prev-entry-link">&larr; <a href="https://blog.jle.im/entry/purely-functional-typed-models-1.html">A Purely Functional Typed Approach to Trainable Models (Part 1)</a> (Previous)</li><li class="next-entry-link">(Next) <a href="https://blog.jle.im/entry/purely-functional-typed-models-3.html">A Purely Functional Typed Approach to Trainable Models (Part 3)</a> &rarr;</li></ul></nav></footer></article><div class="post-entry"><div class="tile"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blog.jle.im/entry/purely-functional-typed-models-2.html';
    this.page.identifier = 'functional-models-2';
};
(function() {
    var d = document, s = d.createElement('script');
    s.src = '//incode.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a><br></noscript><a href="http://disqus.com" class="dsq-brlink">Comments powered by <span class="logo-disqus">Disqus</span></a></div></div></div></div></div><div id="footer-container"><div id="footer-content"><div class="tile"><div class="footer-copyright">&copy; 2018 Justin Le <span class="license-link">(<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/" class="license">CC-BY-NC-ND 3.0</a>)</span></div><div class="footer-follow social-follows"><ul class="social-follows-list"><li><ul class="social-follows-list-social"><li><a class="social-follow-twitter" title="Follow me on Twitter!" href="https://twitter.com/intent/user?user_id=mstk" onclick="window.open(
  &#39;http://twitter.com/intent/user?user_id=907281&#39;,
  &#39;facebook-share-dialog&#39;,
  &#39;width=550,height=520&#39;);
return false;
">Twitter</a></li><li><a class="social-follow-gplus" title="Add me on Google+!" href="https://plus.google.com/+JustinLe">Google+</a></li><li><a class="social-follow-linkedin" title="Connect with me on LinkedIn!" href="https://linkedin.com/in/lejustin">LinkedIn</a></li><li><a class="social-follow-github" title="Fork me on Github!" href="https://github.com/mstksg">Github</a></li><li><a class="social-follow-keybase" title="Track me on Keybase!" href="https://keybase.io/mstksg">Keybase</a></li><li><a class="social-follow-bitcoin" title="Donate via bitcoin!" href="bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU">Bitcoin</a></li></ul></li><li><ul class="social-follows-list-site"><li><a class="social-follow-rss" title="Subscribe to my RSS Feed!" href="http://feeds.feedburner.com/incodeblog">RSS</a></li><li><a class="social-follow-email" title="Subscribe to the mailing list!" href="https://feedburner.google.com/fb/a/mailverify?loc=en_US&amp;uri=incodeblog">Mailing list</a></li></ul></li></ul></div></div></div></div></body></html>