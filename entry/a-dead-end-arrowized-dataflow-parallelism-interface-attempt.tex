\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}


\begin{document}

\% A (Dead End?) Arrowized Dataflow Parallelism Interface Attempt \% Justin Le
\% April 3, 2014

\emph{Originally posted on
\textbf{\href{https://blog.jle.im/entry/a-dead-end-arrowized-dataflow-parallelism-interface-attempt.html}{in
Code}}.}

So I've been having several 'dead end' projects in Haskell recently that I've
sort of just scrapped and move from, but I decided that it might be nice to
document some of them :) For reading back on it later, for people to possibly
help/offer suggestions, for people to peruse and possibly learn something, or
for people to laugh at me. Here is my most recent semi-failure -\/-\/- implicit
dataflow parallelism through an Arrow interface.

tl;dr:

\begin{enumerate}
\tightlist
\item
  Compose parallelizable computations using expressive proc notation.
\item
  Consolidate and join forks to maintain maximum parallelization.
\item
  All data dependencies implicit; allows for nice succinct direct translations
  of normal functions.
\item
  All "parallelizable" functions can also trivially be typechecked and run as
  normal functions, due to arrow polymorphism.
\end{enumerate}

The main problem:

\begin{itemize}
\tightlist
\item
  Consider \texttt{ParArrow\ a\ c}, \texttt{ParArrow\ b\ d},
  \texttt{ParArrow\ (c,d)\ (e,f)}, \texttt{ParArrow\ e\ g}, and
  \texttt{ParArrow\ f\ h}. We execute the first two in parallel, apply the
  third, and execute the second two in parallel. Basically, we want two
  independent \texttt{ParArrow\ a\ g} and \texttt{ParArrow\ c\ h} that we can
  fork. And this is possible, as long as the "middle" arrow does not
  "cross-talk" -\/-\/- that is, it can't be something like
  \texttt{arr\ (\textbackslash{}(x,y)\ -\textgreater{}\ (y,x))}.
\end{itemize}

\section{The Vision}

So what do I mean?

\subsection{Dataflow Parallelism}

By "dataflow parallelism", I refer to structuring parallel computations by "what
depends on what". If two values \emph{can} be computed in parallel, then that is
taken advantage of. Consider something like \texttt{map\ f\ xs}. Normally, this
would: one by one step over \texttt{xs} and apply \texttt{f} to each one,
building a new list as you go along one at a time.

But note that there are some easy places to parallelize this -\/-\/- because
none of the results the mapped list depend on eachother, you can apply
\texttt{f} to every element in parallel, and re-collect everything back at the
end. And this is a big deal if \texttt{f} takes a long time. This is an example
of something commonly refered to as "embarassingly parallel".

\subsection{Arrows}

So what kind of Arrow interface am I imagining with this?

Haskell has some nice syntax for composing "functions" (\texttt{f}, \texttt{g},
and \texttt{h}):

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell proc x
-\textgreater{} do y \textless{}- f -\textless{} x z \textless{}- g -\textless{}
x q \textless{}- h -\textless{} y returnA -\textless{} y * z + q
\textasciitilde{}\textasciitilde{}\textasciitilde{}

A \texttt{proc} statement is a fancy lambda, which takes an input \texttt{x} and
"funnels" \texttt{x} through several different "functions" -\/-\/- in our case,
\texttt{f}, \texttt{g}, and \texttt{h} -\/-\/- and lets you name the results so
that you can use them later.

\includegraphics{/img/entries/pararrow/proc1.png}

While this looks like you are 'performing' \texttt{f}, then \texttt{g}, then
\texttt{h}, what is actually happening is that you are \emph{composing} and
synthesizing a \emph{new function}. You are "assembling" a new function that,
when given an \texttt{x}, collects the results of \texttt{x} run through
\texttt{f}, \texttt{g}, and \texttt{h}, and pops out a function of what comes
out of those functions.

Except...\texttt{f}, \texttt{g}, and \texttt{h} don't have to be normal
functions. They are "generalized" functions; functions that could perhaps even
have side-effects, or trigger special things, or be evaluated in special ways.
They are instances of the \texttt{Arrow} typeclass.

An \texttt{Arrow\ a\ b} just represents, abstractly, a way to get some
\texttt{b} from some \texttt{a}, equipped with combinators that allow you to
compose them in neat ways. Proc notation allows us to assemble a giant new
arrow, from sequencing and composing smaller arrows.

\subsection{Forking Arrows}

Look at the proc statement and tell me that that doesn't scream "data
parallelism" to you. Because every arrow \texttt{f}, \texttt{g}, and \texttt{h}
can potentially do side-effecty, stateful, IO things, depending on how we
implemented the arrow...what if \texttt{f}, \texttt{g}, and \texttt{h}
represented "a way to get a \texttt{b} from an \texttt{a}...in its own separate
thread"?

So if I were to "run" this special arrow, a \texttt{ParArrow\ a\ b}, I would do

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell runPar :: ParArrow a
b -\textgreater{} a -\textgreater{} IO b
\textasciitilde{}\textasciitilde{}\textasciitilde{}

Where if i gave \texttt{runPar} a \texttt{ParArrow\ a\ b}, and an \texttt{a}, It
would fork itself into its own thread and give you an \texttt{IO\ b} in response
to your \texttt{a}.

Because of Arrow's ability to "separate out" and "side-chain" compositions (note
that \texttt{q} in the previous example does not depend on \texttt{z} at all,
and can clearly be launched in parallel alongside the calculation of
\texttt{z}), it looks like from a \texttt{proc} notation statement, we can
easily write arrows that all 'fork themselves' under composition.

Using this, in the above proc example with the fancy diagram, we should be able
to see that \texttt{z} is completely independent of \texttt{y} and \texttt{q},
so the \texttt{g} arrow could really compute itself "in parallel", forked-off,
from the \texttt{f} and \texttt{h} arrows.

You should also be able to "join together" parallel computations. That is, if
you have an \texttt{a\ -\textgreater{}\ c} and a \texttt{b\ -\textgreater{}\ d},
you could make a "parallel" \texttt{(a,b)\ -\textgreater{}\ (c,d)}. But what if
I also had a \texttt{c\ -\textgreater{}\ e} and a
\texttt{d\ -\textgreater{}\ f}? I could chain the entire
\texttt{a}-\texttt{c}-\texttt{e} chain and the \texttt{b}-\texttt{d}-\texttt{f}
chain, and perform both chains in parallel and re-collect things at the end.
That is, a \texttt{(a,b)\ -\textgreater{}\ (c,d)} and a
\texttt{(c,d)\ -\textgreater{}\ (e,f)} should meaningfully compose into a
\texttt{(a,b)\ -\textgreater{}\ (e,f)}, where the left and right sides (the
\texttt{a\ -\textgreater{}\ e} and the \texttt{b\ -\textgreater{}\ f}) are
performed "in parallel" from eachother.

With that in mind, we could even do something like \texttt{parMap}:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell parMap :: ParArrow a
b -\textgreater{} ParArrow {[}a{]} {[}b{]} parMap f = proc input -\textgreater{}
do case input of {[}{]} -\textgreater{} returnA -\textless{} {[}{]} (x:xs)
-\textgreater{} do y \textless{}- f -\textless{} x ys \textless{}- parMap f
-\textless{} xs returnA -\textless{} y:ys
\textasciitilde{}\textasciitilde{}\textasciitilde{}

And because "what depends on what" is so \emph{obviously clear} from proc/do
notation -\/-\/- you know exactly what depends on what, and the graph is already
laid out there for you -\/-\/- and because \texttt{f} is actaully a "smart"
function, with "smart" semantics which can do things like fork threads to solve
itself...this should be great way to structure programs and take advantage of
implicit data parallelism.

\subsubsection{The coolest thing}

Also notice something cool -\/- if leave our proc blocks polymorphic:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell map' :: ArrowChoice r
=\textgreater{} r a b -\textgreater{} r {[}a{]} {[}b{]} map' f = proc input
-\textgreater{} do case input of {[}{]} -\textgreater{} returnA -\textless{}
{[}{]} (x:xs) -\textgreater{} do y \textless{}- f -\textless{} x ys \textless{}-
map' f -\textless{} xs returnA -\textless{} y:ys
\textasciitilde{}\textasciitilde{}\textasciitilde{}

We can now use \texttt{map\textquotesingle{}} as \emph{both} a normal,
sequentual function \emph{and} a parallel, forked computation!

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: map' (arr
(\emph{2)) {[}1..5{]} {[}2,4,6,8,10{]} λ: runPar \$ map' (arr (}2)) {[}1..5{]}
{[}2,4,6,8,10{]} \textasciitilde{}\textasciitilde{}\textasciitilde{}

Yup!

Let's try implementing it, and let's see where things go wrong.

\section{ParArrow}

\subsection{Data and Instances}

Let's start out with our arrow data type:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L12-18
data ParArrow a b = Pure (a -\textgreater{} b) \textbar{} forall z. Seq
(ParArrow a z) (ParArrow z b) \textbar{} forall a1 a2 b1 b2. Par (a
-\textgreater{} (a1, a2)) (ParArrow a1 b1) (ParArrow a2 b2) ((b1, b2)
-\textgreater{} b) \textasciitilde{}\textasciitilde{}\textasciitilde{}

So a \texttt{ParArrow\ a\ b} represents a (pure) paralleizable, forkable
computation that returns a \texttt{b} (as \texttt{IO\ b}) when given an
\texttt{a}.{[}\^{}unsafepio{]}

\begin{itemize}
\item
  \texttt{Pure\ f} wraps a pure function in a \texttt{ParArrow} that computes
  that function in a fork when necessary.
\item
  \texttt{Seq\ f\ g} sequences a \texttt{ParArrow\ a\ z} and a
  \texttt{ParArrow\ z\ b} into a big \texttt{ParArrow\ a\ b}. It reprensents
  composing two forkable functions into one big forkable function, sequentially.
\item
  \texttt{Par\ l\ f\ g\ r} takes two \texttt{ParArrow}s \texttt{f} and
  \texttt{g} of different types and represents the idea of performing them in
  parallel. Of forking them off from eachother and computing them independently,
  and collecting it all together.

  \texttt{l} and \texttt{r} are supposed to be functions that turn the tupled
  inputs/outputs of the parallel computations and makes them fit
  \texttt{ParArrow\ a\ b}. \texttt{r} is kind of supposed to be \texttt{id}, and
  \texttt{l} is supposed to be \texttt{id} (to continue a parallel action) or
  \texttt{\textbackslash{}x\ -\textgreater{}\ (x,x)} (to begin a fork).

  It's a little hacky, and there might be a better way with GADT's and all sorts
  of type/kind-level magic, but it was the way I found that I understood the
  most.

  The main purpose of \texttt{l} and \texttt{r} is to be able to meaningfully
  refer to the two parallel \texttt{ParArrow}s in terms of the \texttt{a} and
  \texttt{b} of the "combined" \texttt{ParArrow}. Otherwise, the two inputs of
  the two parallel \texttt{ParArrow}s don't have anything to do with the input
  type \texttt{a} of the combined \texttt{ParArrow}, and same for output.
\end{itemize}

Okay, let's define a Category instance, that lets us compose \texttt{ParArrow}s:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L20-22
instance Category ParArrow where id = Pure id f . g = Seq g f
\textasciitilde{}\textasciitilde{}\textasciitilde{}

No surprises there, hopefully! Now an Arrow instance:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L24-29
instance Arrow ParArrow where arr = Pure first f = f *** id second g = id *** g
f \&\&\& g = Par (id \&\&\& id) f g id f *** g = Par id f g id
\textasciitilde{}\textasciitilde{}\textasciitilde{}

Also simple enough. Note that \texttt{first} and \texttt{second} are defined in
terms of \texttt{(***)}, instead of the typical way of defining \texttt{second},
\texttt{(\&\&\&)}, and \texttt{(***)} in terms of \texttt{arr} and
\texttt{first}.

\subsection{The Magic}

Now, for the magic -\/-\/- consolidating a big composition of fragmented
\texttt{ParArrow}s into a streamlined simple-as-possible graph:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L31-51
collapse :: ParArrow a b -\textgreater{} ParArrow a b collapse (Seq f g) = case
(collapse f, collapse g) of (Pure p1, Pure p2) -\textgreater{} Pure (p1
\textgreater{}\textgreater{}\textgreater{} p2) (Seq s1 s2, \emph{)
-\textgreater{} Seq (collapse s1) (collapse (Seq s2 g)) (}, Seq s1 s2)
-\textgreater{} Seq (collapse (Seq f s1)) (collapse s2) (Pure p, Par l p1 p2 r)
-\textgreater{} Par (p \textgreater{}\textgreater{}\textgreater{} l) (collapse
p1) (collapse p2) r (Par l p1 p2 r, Pure p) -\textgreater{} Par l (collapse p1)
(collapse p2) (r \textgreater{}\textgreater{}\textgreater{} p) (Par l p1 p2 r,
Par l' p1' p2' r') -\textgreater{} let p1f x = fst . l' . r \$ (x, undefined)
p2f x = snd . l' . r \$ (undefined, x) pp1 = collapse (p1
\textgreater{}\textgreater{}\textgreater{} arr p1f
\textgreater{}\textgreater{}\textgreater{} p1') pp2 = collapse (p2
\textgreater{}\textgreater{}\textgreater{} arr p2f
\textgreater{}\textgreater{}\textgreater{} p2') in Par l pp1 pp2 r' collapse p =
p \textasciitilde{}\textasciitilde{}\textasciitilde{}

There are probably a couple of redundant calls to \texttt{collapse} in there,
but the picture should still be evident:

\begin{itemize}
\item
  Collapsing two sequenced \texttt{Pure}s should just be a single \texttt{Pure}
  with their pure functions composed.
\item
  Collapsing a \texttt{Seq} sequenced with anything else should re-associate the
  \texttt{Seq}s to the left, and collapse the \texttt{ParArrow}s inside as well.
\item
  Collapsing a \texttt{Pure} and a \texttt{Par} should just involve moving the
  function inside the \texttt{Pure} to the wrapping/unwrapping functions around
  the \texttt{Par}.
\item
  Collapsing two \texttt{Par}s is where the fun happens!

  We "fuse" the parallel branches of the fork together. We do that by running
  the export functions and the extract functions on each side, "ignoring" the
  other half of the tuple. This should work if the export/extract functions are
  all either \texttt{id} or \texttt{id\ \&\&\&\ id}.
\end{itemize}

And...here we have a highly condensed parallelism graph.

\subsection{\texorpdfstring{Inspecting \texttt{ParArrow}
structures}{Inspecting ParArrow structures}}

It might be useful to get a peek at the internal structures of a collapsed
\texttt{ParArrow}. I used a helper data type, \texttt{Graph}.

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L76-79
data Graph = GPure -\/- Pure function \textbar{} Graph :-\textgreater{}: Graph
-\/- Sequenced arrows \textbar{} Graph :/: Graph -\/- Parallel arrows deriving
Show \textasciitilde{}\textasciitilde{}\textasciitilde{}

And we can convert a given \texttt{ParArrow} into its internal graph:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L81-87
analyze' :: ParArrow a b -\textgreater{} Graph analyze' (Pure \emph{) = GPure
analyze' (Seq f g) = analyze' f :-\textgreater{}: analyze' g analyze' (Par } f g
\_) = analyze' f :/: analyze' g

analyze :: ParArrow a b -\textgreater{} Graph analyze = analyze' . collapse
\textasciitilde{}\textasciitilde{}\textasciitilde{}

\subsection{Sample ParArrows}

Let's try examining it with some simple \texttt{Arrow}s, like the one we
mentioned before:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: let test1 =
\textbar{} proc x -\textgreater{} do \textbar{} y \textless{}- arr (\emph{2)
-\textless{} x \textbar{} z \textless{}- arr (+3) -\textless{} x \textbar{} q
\textless{}- arr (\^{}2) -\textless{} y \textbar{} returnA -\textless{} y } z +
q λ: :t test1 test1 :: (Arrow r, Num t) =\textgreater{} r t t λ: test1 5 180 λ:
analyze test1 GPure :/: GPure
\textasciitilde{}\textasciitilde{}\textasciitilde{}

This is what we would expect. From looking at the diagram above, we can see that
there are two completely parallel forks; so in the collapsed arrow, there are
indeed only two parallel forks of pure functions.

How about a much simpler one that we unroll ourselves:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: let test2 = arr
(uncurry (+)) \textbar{} . (arr (\emph{2) }** arr (+3)) \textbar{} . (id \&\&\&
id) λ: :t test2 test2 :: (Arrow r, Num t) =\textgreater{} r t t λ: test2 5 18 λ:
analyze' test2 ((GPure :/: GPure) :-\textgreater{}: (GPure :/: GPure))
:-\textgreater{}: GPure λ: analyze test2 GPure :/: GPure
\textasciitilde{}\textasciitilde{}\textasciitilde{}

So as we can see, the "uncollapsed" \texttt{test2} is actually three sequenced
functions (as we would expect): Two parallel pure arrows (the
\texttt{id\ \&\&\&\ id} and \texttt{(arr\ (*2)\ ***\ arr\ (+3))}) and then one
sequential arrow (the \texttt{arr\ (uncurry\ (+))}).

However, we can see that that is just a single fork-and-recombine, so when we
collapse it, we get \texttt{GPure\ :/:\ GPure}, as we would expect.

\section{Running ParArrows}

Now we just need a way to run a \texttt{ParArrow}, and do the proper forking.
This actually isn't too bad at all, because of what we did in \texttt{collapse}.

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L92-113
runPar' :: ParArrow a b -\textgreater{} (a -\textgreater{} IO b) runPar' = go
where go :: ParArrow a b -\textgreater{} (a -\textgreater{} IO b) go (Pure f) =
\textbackslash{}x -\textgreater{} putStrLn "P" \textgreater{}\textgreater{}
return (f x) go (Seq f g) = go f \textgreater{}=\textgreater{} go g go (Par l f
g r) = \textbackslash{}x -\textgreater{} do putStrLn "F"

\begin{verbatim}
  fres <- newEmptyMVar
  gres <- newEmptyMVar

  let (fin,gin) = l x
  forkIO $ runPar' f fin >>= putMVar fres
  forkIO $ runPar' g gin >>= putMVar gres

  reses <- (,) <$> takeMVar fres <*> takeMVar gres
  return (r reses)
\end{verbatim}

runPar :: ParArrow a b -\textgreater{} (a -\textgreater{} IO b) runPar = runPar'
. collapse \textasciitilde{}\textasciitilde{}\textasciitilde{}

(Note that I left in debug traces)

\subsection{Testing}

Sweet, now let's run it!

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: test2 5 18 λ:
runPar test2 5 F P P 18 \textasciitilde{}\textasciitilde{}\textasciitilde{}

That works as expected!

We can see from the debug trace that first things are forked, and then two pure
functions are run. A final value of 18 is returned, which is the same as for the
\texttt{(-\textgreater{})} version. (Note how we can use \texttt{test2} as both,
due to what we mentioned above)

Okay, so it looks like this does exactly what we want. It intelligently "knows"
when to fork, when to unfork, when to "sequence" forks. Let's try it with
\texttt{test1}, which was written in \texttt{proc} notation.

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: test1 5 180 λ:
runPar test1 5 F P P *** Exception: Prelude.undefined
\textasciitilde{}\textasciitilde{}\textasciitilde{}

What! :/

\section{What went wrong}

Let's dig into actual desguaring. According to the proc notation specs:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell test3 = proc x
-\textgreater{} do y \textless{}- arr (*2) -\textless{} x z \textless{}- arr
(+3) -\textless{} x returnA -\textless{} y + z

-\/- desugared: test3' = arr ((x,y) -\textgreater{} x + y) -\/- add . arr ((x,y)
-\textgreater{} (y,x)) -\/- flip . first (arr (+3)) -\/- z . arr ((x,y)
-\textgreater{} (y,x)) -\/- flip . first (arr (*2)) -\/- y . arr
(\textbackslash{}x -\textgreater{} (x,x)) -\/- split
\textasciitilde{}\textasciitilde{}\textasciitilde{}

Ah. Everything is in terms of \texttt{arr} and \texttt{first}, and it never uses
\texttt{second}, \texttt{(***)}, or \texttt{(\&\&\&)}. (These should be
equivalent, due to the Arrow laws, of course; my instance is obviously unlawful,
oops)

I'm going to cut right to the chase here. The main problem is our collapsing
sequenced \texttt{Pure} and \texttt{Par}s.

Basically, the collapsing rules say that if we have:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell Par l p1 p2 r
\texttt{Seq} Pure f \texttt{Seq} Par l' p1' p2' r'
\textasciitilde{}\textasciitilde{}\textasciitilde{}

It should be the same as one giant \texttt{Par}, where \texttt{f} is "injected"
between \texttt{p1} and \texttt{p1\textquotesingle{}}, \texttt{p2} and
\texttt{p2\textquotesingle{}}.

The bridge is basically a tuple, and we take advantage of laziness to basically
pop the results of \texttt{p1} into a tuple using \texttt{r}, apply \texttt{f}
to the tuple, and extract it using \texttt{l}, and run it through
\texttt{p1\textquotesingle{}}.

So \texttt{f} has to be some sort of function
\texttt{(a,b)\ -\textgreater{}\ (c,d)}, where \texttt{c}'s value can only depend
on \texttt{a}'s value, and \texttt{d}'s value can only depend on \texttt{b}'s
value. Basically, it has to be derived from functions
\texttt{a\ -\textgreater{}\ c} and \texttt{b\ -\textgreater{}\ d}. A "parallel"
function.

As long as this is true, this will work.

However, we see in the desugaring of \texttt{test3} that \texttt{f} is not
always that. \texttt{f} can be \emph{any} function, actually, and we can't
really control what happens to it. In \texttt{test3}, we actaully use
\texttt{f\ =\ \textbackslash{}(x,y)\ -\textgreater{}\ (y,x)}...definitely not a
"parallel" function!

Actually, this doesn't even make any sense in terms of our parallel computation
model! How can we "combine" two parallel forks...when halfway in between the two
forks, they must exchange information? Then it's no longer fully parallel!

We can "fix" this. We can make \texttt{collapse} not collapse the
\texttt{Pure}-\texttt{Par} cases:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell -\/- source:
https://github.com/mstksg/inCode/tree/master/code-samples/pararrow/ParArrow.hs\#L53-116
collapse\_ :: ParArrow a b -\textgreater{} ParArrow a b collapse\_ (Seq f g) =
case (collapse\_ f, collapse\_ g) of (Pure p1, Pure p2) -\textgreater{} Pure (p1
\textgreater{}\textgreater{}\textgreater{} p2) (Seq s1 s2, \emph{)
-\textgreater{} Seq (collapse} s1) (collapse\_ (Seq s2 g)) (\emph{, Seq s1 s2)
-\textgreater{} Seq (collapse} (Seq f s1)) (collapse\_ s2) -\/- (Pure p, Par l
p1 p2 r) -\textgreater{} Par (p \textgreater{}\textgreater{}\textgreater{} l)
-\/- (collapse\_ p1) (collapse\_ p2) -\/- r -\/- (Par l p1 p2 r, Pure p)
-\textgreater{} Par l -\/- (collapse\_ p1) (collapse\_ p2) -\/- (r
\textgreater{}\textgreater{}\textgreater{} p) (Par l p1 p2 r, Par l' p1' p2' r')
-\textgreater{} let p1f x = fst . l' . r \$ (x, undefined) p2f x = snd . l' . r
\$ (undefined, x) pp1 = collapse\_ (p1
\textgreater{}\textgreater{}\textgreater{} arr p1f
\textgreater{}\textgreater{}\textgreater{} p1') pp2 = collapse\_ (p2
\textgreater{}\textgreater{}\textgreater{} arr p2f
\textgreater{}\textgreater{}\textgreater{} p2') in Par l pp1 pp2 r' (f,g)
-\textgreater{} Seq f g collapse\_ p = p

analyze\_ :: ParArrow a b -\textgreater{} Graph analyze\_ = analyze' .
collapse\_

runPar\_ :: ParArrow a b -\textgreater{} (a -\textgreater{} IO b) runPar\_ =
runPar' . collapse\_ \textasciitilde{}\textasciitilde{}\textasciitilde{}

Then we have:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: analyze\_ test1 (
GPure :-\textgreater{}: ( ( GPure :-\textgreater{}: GPure ) :/: GPure ) )
:-\textgreater{}: (( GPure :-\textgreater{}: ( ( GPure :-\textgreater{}: GPure )
:/: GPure ) ) :-\textgreater{}: (( GPure :-\textgreater{}: ( ( GPure
:-\textgreater{}: GPure ) :/: GPure ) ) :-\textgreater{}: GPure ))
\textasciitilde{}\textasciitilde{}\textasciitilde{}

We basically have three
\texttt{GPure\ :-\textgreater{}\ ((GPure\ :-\textgreater{}:\ GPure)\ :/:\ GPure)}'s
in a row. A pure function followed by parallel functions. This sort of makes
sense, and if we sort of imagined manually unrolling \texttt{test3}, this is
what we'd imagine we'd get, sorta. now we don't "collapse" the three parallel
forks together.

This runs without error:

\textasciitilde{}\textasciitilde{}\textasciitilde{}haskell λ: runPar\_ test1 5 P
F P P P P F P P P P 18 \textasciitilde{}\textasciitilde{}\textasciitilde{}

And the trace shows that it is "forking" two times. The structural analysis
would actaully suggest that we forked three times, but...I'm not totally sure
what's going on here heh. Still two is much more than what should ideally be
required (one).

\section{Oh.}

So now we can no longer fully "collapse" the two parallel forks, and it involves
forking twice. Which makes complete sense, because we have to swap in the
middle.

And without the collapsing...there are a lot of unecessary
reforks/recominbations that would basically kill any useful parallelization
unless you pre-compose all of your forks...which kind of defeats the purpose of
the implicit dataflow parallization in the first place.

Anyways, this is all rather annoying, because the analogous manual
\texttt{(\&\&\&)} / \texttt{(***)} / \texttt{second}-based \texttt{test1} should
not ever fail, because we never fork. So if the proc block had desugared to
using those combinators and never using
\texttt{arr\ (\textbackslash{}(x,y)\ -\textgreater{}\ (y,x))}, everything would
work out fine!

But hey, if you write out the arrow computation manually by composing
\texttt{(\&\&\&)}, \texttt{(***)}, and \texttt{second}...this will \emph{all
actually work}! I mean serious! Isn't that crazy! (Provided, all of your
\texttt{Pure}'s are sufficiently "parallel").

But the whole point in the first place was to use proc/do notation, so this
becomes a lot less useful than before.

Also, it's inherently pretty fragile, as you can no longer rely on the type
system to enforce "sufficiently parallel" \texttt{Pure}'s. You can't even check
against something like
\texttt{arr\ (\textbackslash{}(x,y)\ -\textgreater{}\ (x,x))}, which makes no
sense again in 'isolated parallel' computations.

(Interestingly enough, you \emph{can} use the type system to enforce against
things like \texttt{arr\ (\textbackslash{}(x,y)\ -\textgreater{}\ x)} or
\texttt{arr\ (\textbackslash{}(x,y)\ -\textgreater{}\ 5)}; you can't collapse
tuples)

Basically, \emph{it mostly works} for almost all
\texttt{ParArrow\ (a,b)\ (c,d)}...\emph{except} for when they have cross-talk.

So, well...back to the drawing board I guess.

\section{What can be done?}

So I'm open to seeing different avenues that this can be approached by, and also
if anyone else has tried doing this and had more success than me.

In particular, I do not have much experience with type-/kind-level stuff
involving those fun extensions, so if there is something that can be done there,
I would be happy to learn :)

\subsection{Other avenues}

I have tried other things "in addition to" the things mentioned in this post,
but most of them have also been dead ends. Among one of the attempts that I
tried involve throwing exceptions from one thread to another containing the
"missing half". If an
\texttt{arr\ (\textbackslash{}(x,y)\ -\textgreater{}\ (y,x))}-like function is
used, then each thread will know, and "wait" on the other to throw an exception
to the other containing the missing data.

I couldn't get this to work, exactly, because I couldn't get it to work without
adding a \texttt{Typeable} constraint to the parameters...and even when using
things like the \href{http://hsenag.livejournal.com/11803.html}{constrained
monads technique}, I couldn't get the "unwrap" functions to work because I
couldn't show that \texttt{z}, \texttt{a1}, \texttt{b1}, etc. were Typeable.

Perhaps without the exception method, I could use \texttt{MVar}s to sort of have
a branch "wait" on the other if they find out that they have been given an
\texttt{arr} that has cross-talk.

Another path is just giving up \texttt{Arrow} completely and using non-typeclass
... but I don't think that offers much advantages over the current system (using
\texttt{(***)} etc.), and also it gives up the entire point -\/-\/- using proc
notation, and also the neat ability to use them as if they were regular
functions.

For now, though, I am calling this a "dead end"{[}\^{}another{]}; if anyone has
any suggestions, I'd be happy to hear them :) I just thought it'd be worth
putting up my thought process up in written form somewhere so that I could look
back on them, or so that people can see what doesn't work and/or possibly learn
:) And of course for entertainment in case I am hilariously awful.

\end{document}
